{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jV-qaRjjh51v"
   },
   "source": [
    "# Hybrid Recommendation System\n",
    "\n",
    "**Team Structure:**\n",
    "- Member 1: Infrastructure, Data Loading, Fusion & Evaluation\n",
    "- Member 2: Collaborative Filtering (ALS)\n",
    "- Member 3: Content-Based Filtering (TF-IDF + LSH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z3aeJVqkPIa"
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnzLbX57kUnJ"
   },
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GMg3a2fqh51y",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:27.823551Z",
     "start_time": "2026-01-18T14:39:27.805055Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from math import log2\n",
    "\n",
    "# Fix for Windows\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, FloatType, StructType, StructField"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d--B_svch51z"
   },
   "source": [
    "### 1.2 Download Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7xDJxcgHh511",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:27.862327Z",
     "start_time": "2026-01-18T14:39:27.827819Z"
    }
   },
   "source": [
    "DATA_URL = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
    "DATA_DIR = \"data\"\n",
    "DATASET_DIR = os.path.join(DATA_DIR, \"ml-1m\")\n",
    "ZIP_PATH = os.path.join(DATA_DIR, \"ml-1m.zip\")"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2xaxdp_xh512",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:27.912937Z",
     "start_time": "2026-01-18T14:39:27.882058Z"
    }
   },
   "source": [
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t3cKFv2Bh513",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:27.950271Z",
     "start_time": "2026-01-18T14:39:27.915998Z"
    }
   },
   "source": [
    "if not os.path.exists(DATASET_DIR):\n",
    "\n",
    "    if not os.path.exists(ZIP_PATH):\n",
    "        print(\"Downloading MovieLens ml-1m...\")\n",
    "        urllib.request.urlretrieve(DATA_URL, ZIP_PATH)\n",
    "\n",
    "    print(\"Extracting...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR)"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew2EJ0KCh513"
   },
   "source": [
    "### 1.3 Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zH6tSX_bh513",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:27.967186Z",
     "start_time": "2026-01-18T14:39:27.951847Z"
    }
   },
   "source": [
    "spark = SparkSession.builder.appName(\"MMDS\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"20\") \\\n",
    "    .config(\"spark.default.parallelism\", \"20\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"400m\") \\\n",
    "    .getOrCreate()"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y95TFnL2h514"
   },
   "source": [
    "### 1.4 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBLTLe1th514",
    "outputId": "fc2b2efd-dfe3-4498-e34a-ba1de16a3140",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:28.097252Z",
     "start_time": "2026-01-18T14:39:27.976759Z"
    }
   },
   "source": [
    "users_df = spark.read.text(os.path.join(DATASET_DIR, \"users.dat\")).select(\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(0).cast(IntegerType()).alias(\"user_id\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(1).alias(\"gender\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(2).cast(IntegerType()).alias(\"age\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(3).cast(IntegerType()).alias(\"occupation\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(4).alias(\"zip_code\")\n",
    ")\n",
    "users_df.count()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6040"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFLr1EX2h514",
    "outputId": "08e9e755-c1b5-4160-d625-643f128fc28e",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:28.173016Z",
     "start_time": "2026-01-18T14:39:28.098385Z"
    }
   },
   "source": [
    "items_df = spark.read.text(os.path.join(DATASET_DIR, \"movies.dat\")).select(\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(0).cast(IntegerType()).alias(\"item_id\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(1).alias(\"title\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(2).alias(\"genres\")\n",
    ")\n",
    "items_df.count()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uafyQhs3h515",
    "outputId": "1af79a1e-18be-442c-d72f-fa38cf29fbed",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:28.278341Z",
     "start_time": "2026-01-18T14:39:28.190733Z"
    }
   },
   "source": [
    "ratings_df = spark.read.text(os.path.join(DATASET_DIR, \"ratings.dat\")).select(\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(0).cast(IntegerType()).alias(\"user_id\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(1).cast(IntegerType()).alias(\"item_id\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(2).cast(FloatType()).alias(\"rating\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(3).cast(IntegerType()).alias(\"timestamp\")\n",
    ")\n",
    "ratings_df.count()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000209"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36xJVL9Ih515",
    "outputId": "30aaf041-8590-4453-c239-224def2157c4",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:28.392163Z",
     "start_time": "2026-01-18T14:39:28.278984Z"
    }
   },
   "source": [
    "users_df.show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+----------+--------+\n",
      "|user_id|gender|age|occupation|zip_code|\n",
      "+-------+------+---+----------+--------+\n",
      "|      1|     F|  1|        10|   48067|\n",
      "|      2|     M| 56|        16|   70072|\n",
      "|      3|     M| 25|        15|   55117|\n",
      "|      4|     M| 45|         7|   02460|\n",
      "|      5|     M| 25|        20|   55455|\n",
      "+-------+------+---+----------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtXD-ucvh515",
    "outputId": "fe9aaea5-5745-46bc-aa52-f8438ba62fe5",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:28.546704Z",
     "start_time": "2026-01-18T14:39:28.433742Z"
    }
   },
   "source": [
    "items_df.show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|item_id|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Animation|Childre...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|        Comedy|Drama|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BI5_IghMh515",
    "outputId": "e390641c-b757-445d-af40-8b20d25ed3a9",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:28.658983Z",
     "start_time": "2026-01-18T14:39:28.580188Z"
    }
   },
   "source": [
    "ratings_df.show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+---------+\n",
      "|user_id|item_id|rating|timestamp|\n",
      "+-------+-------+------+---------+\n",
      "|      1|   1193|   5.0|978300760|\n",
      "|      1|    661|   3.0|978302109|\n",
      "|      1|    914|   3.0|978301968|\n",
      "|      1|   3408|   4.0|978300275|\n",
      "|      1|   2355|   5.0|978824291|\n",
      "+-------+-------+------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYsOYCQEh515"
   },
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmGEbwy3mDDY"
   },
   "source": [
    "### 2.1 Rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cARZQAvGh515",
    "outputId": "aa9e00a1-9cec-4884-c3e8-bce725927b73",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:28.870478Z",
     "start_time": "2026-01-18T14:39:28.695109Z"
    }
   },
   "source": [
    "num_users = users_df.count()\n",
    "num_items = items_df.count()\n",
    "num_ratings = ratings_df.count()\n",
    "sparsity = (1 - (num_ratings / (num_users * num_items))) * 100\n",
    "\n",
    "print(f\"Users:            {num_users:,}\")\n",
    "print(f\"Movies:           {num_items:,}\")\n",
    "print(f\"Ratings:          {num_ratings:,}\")\n",
    "print(f\"Sparsity:         {sparsity:.2f}%\")\n",
    "print(f\"Avg ratings/user: {num_ratings/num_users:.1f}\")\n",
    "print(f\"Avg ratings/movie:{num_ratings/num_items:.1f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users:            6,040\n",
      "Movies:           3,883\n",
      "Ratings:          1,000,209\n",
      "Sparsity:         95.74%\n",
      "Avg ratings/user: 165.6\n",
      "Avg ratings/movie:257.6\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQaH_GJVmLpj"
   },
   "source": [
    "### 2.2 Rating distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlDMVOJNh515",
    "outputId": "dd79f2eb-46dd-4966-cbbc-0daebd89857e",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:29.886829Z",
     "start_time": "2026-01-18T14:39:28.871027Z"
    }
   },
   "source": [
    "ratings_df.groupBy(\"rating\").count().orderBy(\"rating\").show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48672:>                                                      (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|rating| count|\n",
      "+------+------+\n",
      "|   1.0| 56174|\n",
      "|   2.0|107557|\n",
      "|   3.0|261197|\n",
      "|   4.0|348971|\n",
      "|   5.0|226310|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMl6TOXvmQiN"
   },
   "source": [
    "### 2.3 Genre distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4uolq8hkh515",
    "outputId": "8a63ec2b-7dc1-4c5a-9a12-390dbc0b070c",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:30.042741Z",
     "start_time": "2026-01-18T14:39:29.898443Z"
    }
   },
   "source": [
    "items_df.select(F.explode(F.split(F.col(\"genres\"), \"\\\\|\")).alias(\"genre\")) \\\n",
    "    .groupBy(\"genre\").count().orderBy(F.desc(\"count\")).show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|      genre|count|\n",
      "+-----------+-----+\n",
      "|      Drama| 1603|\n",
      "|     Comedy| 1200|\n",
      "|     Action|  503|\n",
      "|   Thriller|  492|\n",
      "|    Romance|  471|\n",
      "|     Horror|  343|\n",
      "|  Adventure|  283|\n",
      "|     Sci-Fi|  276|\n",
      "| Children's|  251|\n",
      "|      Crime|  211|\n",
      "|        War|  143|\n",
      "|Documentary|  127|\n",
      "|    Musical|  114|\n",
      "|    Mystery|  106|\n",
      "|  Animation|  105|\n",
      "|    Western|   68|\n",
      "|    Fantasy|   68|\n",
      "|  Film-Noir|   44|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M4BhkBOmoPr"
   },
   "source": [
    "### 2.4 User gender distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWtkOG1rh515",
    "outputId": "1955049f-0ef8-40c7-8568-765dfc3d126c",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:30.206365Z",
     "start_time": "2026-01-18T14:39:30.075734Z"
    }
   },
   "source": [
    "users_df.groupBy(\"gender\").count().show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     F| 1709|\n",
      "|     M| 4331|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_t-YXlxmtrw"
   },
   "source": [
    "### 2.5 User age distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tt5JfAqKh515",
    "outputId": "5e18a191-b997-4b90-fcb5-9576ecaafb17",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:30.469168Z",
     "start_time": "2026-01-18T14:39:30.298618Z"
    }
   },
   "source": [
    "users_df.groupBy(\"age\").count().orderBy(\"age\").show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "|  1|  222|\n",
      "| 18| 1103|\n",
      "| 25| 2096|\n",
      "| 35| 1193|\n",
      "| 45|  550|\n",
      "| 50|  496|\n",
      "| 56|  380|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWmkYV91h516"
   },
   "source": [
    "## 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r2lL2tL-h516",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:30.548760Z",
     "start_time": "2026-01-18T14:39:30.524396Z"
    }
   },
   "source": [
    "def chronological_split(ratings_df, train_ratio=0.8, min_train_ratings=5):\n",
    "    user_time_window = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "    user_count_window = Window.partitionBy(\"user_id\")\n",
    "\n",
    "    ratings_with_rank = ratings_df.withColumn(\n",
    "        \"row_num\", F.row_number().over(user_time_window)\n",
    "    ).withColumn(\n",
    "        \"user_total\", F.count(\"*\").over(user_count_window)\n",
    "    ).withColumn(\n",
    "        \"train_threshold\", F.floor(F.col(\"user_total\") * train_ratio)\n",
    "    )\n",
    "\n",
    "    ratings_valid = ratings_with_rank.filter(\n",
    "        F.col(\"train_threshold\") >= min_train_ratings\n",
    "    )\n",
    "\n",
    "    ratings_labeled = ratings_valid.withColumn(\n",
    "        \"split\",\n",
    "        F.when(F.col(\"row_num\") <= F.col(\"train_threshold\"), \"train\").otherwise(\"test\")\n",
    "    )\n",
    "\n",
    "    original_columns = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
    "    train_df = ratings_labeled.filter(F.col(\"split\") == \"train\").select(original_columns)\n",
    "    test_df = ratings_labeled.filter(F.col(\"split\") == \"test\").select(original_columns)\n",
    "\n",
    "    return train_df, test_df"
   ],
   "outputs": [],
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7bz6lAKh516",
    "outputId": "43cfc1af-6a46-4cdf-f794-ca94e38ad55a",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:30.602885Z",
     "start_time": "2026-01-18T14:39:30.553906Z"
    }
   },
   "source": [
    "train_df, test_df = chronological_split(ratings_df, train_ratio=0.8, min_train_ratings=5)\n",
    "train_df = train_df.cache()\n",
    "test_df = test_df.cache()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/18 16:39:30 WARN CacheManager: Asked to cache already cached data.\n",
      "26/01/18 16:39:30 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ambqvc_Fh516",
    "outputId": "a85471a3-77c0-4640-d912-043c97d3bca4",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:30.890935Z",
     "start_time": "2026-01-18T14:39:30.603561Z"
    }
   },
   "source": [
    "print(f\"Train: {train_df.count():,} ratings\")\n",
    "print(f\"Test: {test_df.count():,} ratings\")\n",
    "print(f\"Users in train: {train_df.select('user_id').distinct().count():,}\")\n",
    "print(f\"Users in test: {test_df.select('user_id').distinct().count():,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 797,758 ratings\n",
      "Test: 202,451 ratings\n",
      "Users in train: 6,040\n",
      "Users in test: 6,040\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:30.895673Z",
     "start_time": "2026-01-18T14:39:30.892194Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Evaluation Code\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.1 Hyperparameters"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:30.900490Z",
     "start_time": "2026-01-18T14:39:30.896041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "K = 10\n",
    "RELEVANCE_THRESHOLD = 4.0"
   ],
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 Ground Truth"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:30.912493Z",
     "start_time": "2026-01-18T14:39:30.900864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ground_truth = test_df.filter(F.col(\"rating\") >= RELEVANCE_THRESHOLD) \\\n",
    "    .groupBy(\"user_id\").agg(F.collect_list(\"item_id\").alias(\"relevant_items\"))"
   ],
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.3 Functions"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:30.918083Z",
     "start_time": "2026-01-18T14:39:30.913572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_top_k(recs_df, score_col, k):\n",
    "    window = Window.partitionBy(\"user_id\").orderBy(F.desc(score_col))\n",
    "    ranked = recs_df.withColumn(\"rank\", F.row_number().over(window)) \\\n",
    "        .filter(F.col(\"rank\") <= k)\n",
    "\n",
    "    return ranked.groupBy(\"user_id\").agg(\n",
    "        F.array_sort(F.collect_list(F.struct(\"rank\", \"item_id\"))).alias(\"ranked_structs\")\n",
    "    ).withColumn(\n",
    "        \"recommended_items\",\n",
    "        F.expr(\"transform(ranked_structs, x -> x.item_id)\")\n",
    "    ).drop(\"ranked_structs\")"
   ],
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:30.923014Z",
     "start_time": "2026-01-18T14:39:30.918485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def precision_at_k(top_k_df, ground_truth_df, k):\n",
    "    joined = top_k_df.join(ground_truth_df, \"user_id\")\n",
    "    result = joined.withColumn(\"hits\", F.size(F.array_intersect(\"recommended_items\", \"relevant_items\"))) \\\n",
    "        .agg(F.avg(F.col(\"hits\") / k)).collect()[0][0]\n",
    "    return result or 0.0"
   ],
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:30.927981Z",
     "start_time": "2026-01-18T14:39:30.923262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def recall_at_k(top_k_df, ground_truth_df):\n",
    "    joined = top_k_df.join(ground_truth_df, \"user_id\")\n",
    "    result = joined.withColumn(\"hits\", F.size(F.array_intersect(\"recommended_items\", \"relevant_items\"))) \\\n",
    "        .withColumn(\"recall\", F.when(F.size(\"relevant_items\") > 0, F.col(\"hits\") / F.size(\"relevant_items\")).otherwise(0)) \\\n",
    "        .agg(F.avg(\"recall\")).collect()[0][0]\n",
    "    return result or 0.0"
   ],
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:30.933498Z",
     "start_time": "2026-01-18T14:39:30.928247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ndcg_at_k(top_k_df, ground_truth_df, k):\n",
    "    joined = top_k_df.join(ground_truth_df, \"user_id\")\n",
    "    exploded = joined.select(\n",
    "        \"user_id\",\n",
    "        \"relevant_items\",\n",
    "        F.posexplode(\"recommended_items\").alias(\"pos\", \"item_id\")\n",
    "    ).withColumn(\"item_id\", F.col(\"item_id\").cast(IntegerType()))\n",
    "\n",
    "    with_dcg = exploded \\\n",
    "        .withColumn(\"rel\", F.when(F.array_contains(\"relevant_items\", F.col(\"item_id\")), 1.0).otherwise(0.0)) \\\n",
    "        .withColumn(\"dcg\", F.col(\"rel\") / F.log2(F.col(\"pos\") + 2)) \\\n",
    "        .groupBy(\"user_id\", \"relevant_items\").agg(F.sum(\"dcg\").alias(\"dcg\"))\n",
    "\n",
    "    idcg_vals = [sum(1.0 / log2(i + 2) for i in range(n)) for n in range(k + 1)]\n",
    "    idcg_map = F.create_map(*[x for i, v in enumerate(idcg_vals) for x in (F.lit(i), F.lit(v))])\n",
    "\n",
    "    result = with_dcg \\\n",
    "        .withColumn(\"num_rel\", F.least(F.size(\"relevant_items\"), F.lit(k))) \\\n",
    "        .withColumn(\"idcg\", idcg_map[F.col(\"num_rel\")]) \\\n",
    "        .withColumn(\"ndcg\", F.when(F.col(\"idcg\") > 0, F.col(\"dcg\") / F.col(\"idcg\")).otherwise(0)) \\\n",
    "        .agg(F.avg(\"ndcg\")).collect()[0][0]\n",
    "    return result or 0.0"
   ],
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:30.938148Z",
     "start_time": "2026-01-18T14:39:30.933942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(recs_df, score_col, name):\n",
    "    if recs_df.count() == 0:\n",
    "        print(f\"{name}: No recommendations (not implemented)\")\n",
    "        return {\"Precision@10\": 0.0, \"Recall@10\": 0.0, \"NDCG@10\": 0.0}\n",
    "\n",
    "    top_k = get_top_k(recs_df, score_col, K)\n",
    "    p = precision_at_k(top_k, ground_truth, K)\n",
    "    r = recall_at_k(top_k, ground_truth)\n",
    "    n = ndcg_at_k(top_k, ground_truth, K)\n",
    "\n",
    "    print(f\"{name}: P@{K}={p:.4f}, R@{K}={r:.4f}, NDCG@{K}={n:.4f}\")\n",
    "    return {\"Precision@10\": p, \"Recall@10\": r, \"NDCG@10\": n}"
   ],
   "outputs": [],
   "execution_count": 97
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZBXPcymh517"
   },
   "source": [
    "## 5. Collaborative Filtering (ALS)\n",
    "\n",
    "Implement using `pyspark.ml.recommendation.ALS`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tR9qfStQh517",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:30.948116Z",
     "start_time": "2026-01-18T14:39:30.938385Z"
    }
   },
   "source": [
    "class CollaborativeFilter:\n",
    "\n",
    "    def __init__(self, rank=10, regParam=0.1, maxIter=10):\n",
    "        self.als = ALS(\n",
    "            userCol=\"user_id\",\n",
    "            itemCol=\"item_id\",\n",
    "            ratingCol=\"rating\",\n",
    "            rank=rank,\n",
    "            regParam=regParam,\n",
    "            maxIter=maxIter,\n",
    "            coldStartStrategy=\"drop\",\n",
    "            nonnegative=True\n",
    "        )\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, df):\n",
    "        self.model = self.als.fit(df)\n",
    "\n",
    "    def get_recommendations(self, df, k=10):\n",
    "        \"\"\"Get top-K recommendations\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Call train() first.\")\n",
    "\n",
    "        users = df.select(\"user_id\").distinct()\n",
    "        user_recs = self.model.recommendForUserSubset(users, k)\n",
    "\n",
    "        return user_recs.select(\n",
    "            F.col(\"user_id\"),\n",
    "            F.explode(\"recommendations\").alias(\"rec\")\n",
    "        ).select(\n",
    "            F.col(\"user_id\"),\n",
    "            F.col(\"rec.item_id\").cast(IntegerType()).alias(\"item_id\"),\n",
    "            F.col(\"rec.rating\").alias(\"prediction\")\n",
    "        )\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"Predict ratings for user-item pairs in test\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Train should be called first\")\n",
    "        return self.model.transform(df)"
   ],
   "outputs": [],
   "execution_count": 98
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxfposUDh517"
   },
   "source": [
    "### Bonus: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bmVnX8K7h517",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:39:31.115339Z",
     "start_time": "2026-01-18T14:39:30.955547Z"
    }
   },
   "source": [
    "#\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "# --- Inner split for tuning (chronological, like your main split)\n",
    "train_inner, val_df = chronological_split(train_df, train_ratio=0.9, min_train_ratings=5)\n",
    "\n",
    "# Ground truth = relevant in our validation set\n",
    "val_ground_truth = val_df.filter(F.col(\"rating\") >= RELEVANCE_THRESHOLD) \\\n",
    "    .groupBy(\"user_id\").agg(F.collect_list(\"item_id\").alias(\"relevant_items\"))\n",
    "\n",
    "# Cache\n",
    "train_inner.cache()\n",
    "val_df.cache()\n",
    "_ = train_inner.count()\n",
    "_ = val_df.count()\n",
    "\n",
    "def eval_als_on_val(rank, regParam, maxIter, k_candidates=100):\n",
    "    \"\"\"\n",
    "    Train ALS on train_inner, recommend on val_df, compute ranking metrics\n",
    "    vs val_ground_truth.\n",
    "    \"\"\"\n",
    "    cf = CollaborativeFilter(rank=rank, regParam=regParam, maxIter=maxIter)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    cf.train(train_inner)\n",
    "    train_s = time.perf_counter() - t0\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    recs = cf.get_recommendations(val_df, k=k_candidates).withColumnRenamed(\"prediction\", \"als_score\").cache()\n",
    "    _ = recs.count()  # materialize\n",
    "    infer_s = time.perf_counter() - t0\n",
    "\n",
    "    # metrics@K\n",
    "    top_k = get_top_k(recs, \"als_score\", K)\n",
    "    p = precision_at_k(top_k, val_ground_truth, K)\n",
    "    r = recall_at_k(top_k, val_ground_truth)\n",
    "    n = ndcg_at_k(top_k, val_ground_truth, K)\n",
    "\n",
    "    return {\n",
    "        \"rank\": rank, \"regParam\": regParam, \"maxIter\": maxIter,\n",
    "        \"P@K\": p, \"R@K\": r, \"NDCG@K\": n,\n",
    "        \"train_s\": train_s, \"infer_s\": infer_s,\n",
    "    }"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/18 16:39:30 WARN CacheManager: Asked to cache already cached data.\n",
      "26/01/18 16:39:30 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:42:55.384723Z",
     "start_time": "2026-01-18T14:39:31.115842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ranks = [10, 20, 30, 40]\n",
    "regParams = [0.01, 0.05, 0.1, 0.2]\n",
    "maxIters = [10, 20, 30]\n",
    "\n",
    "als_tune_results = []\n",
    "als_best = None\n",
    "\n",
    "for rank, reg, iters in product(ranks, regParams, maxIters):\n",
    "    out = eval_als_on_val(rank, reg, iters, k_candidates=100)\n",
    "    als_tune_results.append(out)\n",
    "\n",
    "    if als_best is None or out[\"NDCG@K\"] > als_best[\"NDCG@K\"]:\n",
    "        als_best = out\n",
    "\n",
    "    print(\n",
    "        f\"ALS tune rank={rank:>2}, reg={reg:<4}, iters={iters:<2} | \"\n",
    "        f\"NDCG@{K}={out['NDCG@K']:.4f} P@{K}={out['P@K']:.4f} R@{K}={out['R@K']:.4f} | \"\n",
    "        f\"train={out['train_s']:.2f}s infer={out['infer_s']:.2f}s\"\n",
    "    )\n",
    "\n",
    "print(\"\\nBEST ALS PARAMS (by NDCG@K):\")\n",
    "print(als_best)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=10, reg=0.01, iters=10 | NDCG@10=0.0005 P@10=0.0004 R@10=0.0006 | train=1.42s infer=1.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=10, reg=0.01, iters=20 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0009 | train=1.85s infer=1.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=10, reg=0.01, iters=30 | NDCG@10=0.0009 P@10=0.0010 R@10=0.0012 | train=2.36s infer=1.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=10, reg=0.05, iters=10 | NDCG@10=0.0030 P@10=0.0028 R@10=0.0047 | train=0.95s infer=1.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=10, reg=0.05, iters=20 | NDCG@10=0.0052 P@10=0.0047 R@10=0.0072 | train=1.57s infer=1.15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=10, reg=0.05, iters=30 | NDCG@10=0.0064 P@10=0.0057 R@10=0.0086 | train=2.22s infer=1.16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=10, reg=0.1 , iters=10 | NDCG@10=0.0037 P@10=0.0035 R@10=0.0062 | train=0.91s infer=1.14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=10, reg=0.1 , iters=20 | NDCG@10=0.0069 P@10=0.0063 R@10=0.0105 | train=1.55s infer=1.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=10, reg=0.1 , iters=30 | NDCG@10=0.0078 P@10=0.0071 R@10=0.0116 | train=2.24s infer=1.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=10, reg=0.2 , iters=10 | NDCG@10=0.0003 P@10=0.0003 R@10=0.0004 | train=0.93s infer=1.08s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=10, reg=0.2 , iters=20 | NDCG@10=0.0006 P@10=0.0006 R@10=0.0011 | train=1.56s infer=1.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=10, reg=0.2 , iters=30 | NDCG@10=0.0006 P@10=0.0007 R@10=0.0012 | train=2.20s infer=1.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=20, reg=0.01, iters=10 | NDCG@10=0.0016 P@10=0.0017 R@10=0.0013 | train=1.31s infer=1.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=20, reg=0.01, iters=20 | NDCG@10=0.0023 P@10=0.0023 R@10=0.0023 | train=2.28s infer=1.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=20, reg=0.01, iters=30 | NDCG@10=0.0026 P@10=0.0027 R@10=0.0026 | train=3.28s infer=1.22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=20, reg=0.05, iters=10 | NDCG@10=0.0087 P@10=0.0069 R@10=0.0113 | train=1.29s infer=1.15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=20, reg=0.05, iters=20 | NDCG@10=0.0104 P@10=0.0083 R@10=0.0129 | train=2.22s infer=1.19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=20, reg=0.05, iters=30 | NDCG@10=0.0110 P@10=0.0086 R@10=0.0135 | train=3.14s infer=1.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=20, reg=0.1 , iters=10 | NDCG@10=0.0068 P@10=0.0062 R@10=0.0105 | train=1.21s infer=1.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=20, reg=0.1 , iters=20 | NDCG@10=0.0100 P@10=0.0087 R@10=0.0142 | train=2.02s infer=1.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=20, reg=0.1 , iters=30 | NDCG@10=0.0112 P@10=0.0094 R@10=0.0151 | train=2.98s infer=1.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=20, reg=0.2 , iters=10 | NDCG@10=0.0005 P@10=0.0005 R@10=0.0009 | train=1.15s infer=1.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=20, reg=0.2 , iters=20 | NDCG@10=0.0006 P@10=0.0007 R@10=0.0012 | train=1.93s infer=1.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=20, reg=0.2 , iters=30 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=2.79s infer=1.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=30, reg=0.01, iters=10 | NDCG@10=0.0033 P@10=0.0031 R@10=0.0027 | train=1.65s infer=1.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=30, reg=0.01, iters=20 | NDCG@10=0.0042 P@10=0.0037 R@10=0.0041 | train=3.02s infer=1.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=30, reg=0.01, iters=30 | NDCG@10=0.0044 P@10=0.0040 R@10=0.0041 | train=4.52s infer=1.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=30, reg=0.05, iters=10 | NDCG@10=0.0118 P@10=0.0093 R@10=0.0145 | train=1.53s infer=1.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=30, reg=0.05, iters=20 | NDCG@10=0.0142 P@10=0.0106 R@10=0.0168 | train=3.06s infer=1.27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=30, reg=0.05, iters=30 | NDCG@10=0.0143 P@10=0.0109 R@10=0.0165 | train=4.43s infer=1.16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=30, reg=0.1 , iters=10 | NDCG@10=0.0079 P@10=0.0069 R@10=0.0121 | train=1.63s infer=1.16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=30, reg=0.1 , iters=20 | NDCG@10=0.0111 P@10=0.0094 R@10=0.0155 | train=2.94s infer=1.35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=30, reg=0.1 , iters=30 | NDCG@10=0.0122 P@10=0.0103 R@10=0.0165 | train=3.89s infer=1.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=30, reg=0.2 , iters=10 | NDCG@10=0.0004 P@10=0.0004 R@10=0.0006 | train=1.46s infer=1.12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=30, reg=0.2 , iters=20 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=2.52s infer=1.37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=30, reg=0.2 , iters=30 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=4.29s infer=1.63s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=40, reg=0.01, iters=10 | NDCG@10=0.0038 P@10=0.0037 R@10=0.0036 | train=2.24s infer=1.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=40, reg=0.01, iters=20 | NDCG@10=0.0060 P@10=0.0054 R@10=0.0054 | train=3.96s infer=1.32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=40, reg=0.01, iters=30 | NDCG@10=0.0060 P@10=0.0053 R@10=0.0058 | train=6.45s infer=2.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=40, reg=0.05, iters=10 | NDCG@10=0.0143 P@10=0.0109 R@10=0.0164 | train=2.17s infer=1.24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=40, reg=0.05, iters=20 | NDCG@10=0.0153 P@10=0.0115 R@10=0.0176 | train=3.72s infer=1.41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=40, reg=0.05, iters=30 | NDCG@10=0.0158 P@10=0.0118 R@10=0.0181 | train=5.56s infer=1.57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=40, reg=0.1 , iters=10 | NDCG@10=0.0087 P@10=0.0073 R@10=0.0130 | train=2.01s infer=1.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=40, reg=0.1 , iters=20 | NDCG@10=0.0120 P@10=0.0099 R@10=0.0156 | train=3.54s infer=1.39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=40, reg=0.1 , iters=30 | NDCG@10=0.0128 P@10=0.0104 R@10=0.0165 | train=4.99s infer=1.26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=40, reg=0.2 , iters=10 | NDCG@10=0.0005 P@10=0.0004 R@10=0.0009 | train=1.73s infer=1.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=40, reg=0.2 , iters=20 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=3.22s infer=1.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS tune rank=40, reg=0.2 , iters=30 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=5.04s infer=1.41s\n",
      "\n",
      "BEST ALS PARAMS (by NDCG@K):\n",
      "{'rank': 40, 'regParam': 0.05, 'maxIter': 30, 'P@K': 0.011795316565481412, 'R@K': 0.018100251078114492, 'NDCG@K': 0.015804939437897077, 'train_s': 5.560506457986776, 'infer_s': 1.5690752500086091}\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:43:03.119149Z",
     "start_time": "2026-01-18T14:42:55.398569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrain final ALS on full train_df with the best params\n",
    "cf = CollaborativeFilter(rank=als_best[\"rank\"], regParam=als_best[\"regParam\"], maxIter=als_best[\"maxIter\"])\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "cf.train(train_df)\n",
    "als_train_s = time.perf_counter() - t0\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "als_recs = cf.get_recommendations(test_df, k=100).withColumnRenamed(\"prediction\", \"als_score\").cache()\n",
    "als_num_recs = als_recs.count()\n",
    "als_infer_s = time.perf_counter() - t0\n",
    "\n",
    "print(f\"\\nFINAL ALS train time: {als_train_s:.2f}s\")\n",
    "print(f\"FINAL ALS inference time: {als_infer_s:.2f}s | rec rows: {als_num_recs:,}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 83047:==========================================>        (83 + 10) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL ALS train time: 6.22s\n",
      "FINAL ALS inference time: 1.45s | rec rows: 604,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:43:03.314763Z",
     "start_time": "2026-01-18T14:43:03.136601Z"
    }
   },
   "cell_type": "code",
   "source": "als_recs.show(10)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+\n",
      "|user_id|item_id|als_score|\n",
      "+-------+-------+---------+\n",
      "|     95|   1198| 4.681128|\n",
      "|     95|    260|4.6353135|\n",
      "|     95|    318| 4.516557|\n",
      "|     95|   2028|4.5126915|\n",
      "|     95|   1197| 4.474654|\n",
      "|     95|    953| 4.467466|\n",
      "|     95|    110|4.4306154|\n",
      "|     95|   3338|4.4078646|\n",
      "|     95|    912| 4.383786|\n",
      "|     95|     37|4.3645916|\n",
      "+-------+-------+---------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMWQPlANh517"
   },
   "source": [
    "## 6. Content-Based Filtering (TF-IDF + LSH)\n",
    "\n",
    "Implement using `pyspark.ml.feature` (Tokenizer, HashingTF, IDF, BucketedRandomProjectionLSH)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:43:18.261643Z",
     "start_time": "2026-01-18T14:43:03.366699Z"
    }
   },
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, NGram, MinHashLSH\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import (\n",
    "    col, split, concat_ws, regexp_extract, udf, array_intersect, \n",
    "    size, sum as _sum, desc, row_number\n",
    ")\n",
    "\n",
    "class ContentBasedFilter:\n",
    "    def __init__(self, num_features=5000, bigram_features=3000, num_hash_tables=10):\n",
    "        self.num_features = num_features\n",
    "        self.bigram_features = bigram_features\n",
    "        self.num_hash_tables = num_hash_tables\n",
    "        self.jaccard_threshold = 0.1\n",
    "        self.genre_weight = 4\n",
    "        self.min_genre_overlap = 1\n",
    "        \n",
    "\n",
    "        self.lsh_model = None\n",
    "        self.vector_model = None\n",
    "        self.movies_binary = None\n",
    "        self.similar_pairs = None\n",
    "\n",
    "    def combine_binarize_udf(self):\n",
    "        def process_vectors(v1, v2):\n",
    "            if v1 is None: \n",
    "                return None\n",
    "            indices = [int(i) for i in v1.indices]\n",
    "            \n",
    "            if v2 is not None:\n",
    "                offset = int(v1.size)\n",
    "                indices += [int(i) + offset for i in v2.indices]\n",
    "                total_size = offset + int(v2.size)\n",
    "            else:\n",
    "                total_size = int(v1.size)\n",
    "            \n",
    "            values = [1.0] * len(indices)\n",
    "            return Vectors.sparse(total_size, sorted(indices), values)\n",
    "            \n",
    "        return udf(process_vectors, VectorUDT())\n",
    "\n",
    "    def train_features(self, items_df):\n",
    "        print(\"building content features (tfidf + bigrams)\")\n",
    "        \n",
    "        df = (\n",
    "            items_df\n",
    "            .withColumn(\"year\", regexp_extract(col(\"title\"), r\"\\((\\d{4})\\)\", 1))\n",
    "            .withColumn(\"decade\", regexp_extract(col(\"title\"), r\"\\((\\d{3})\\d\\)\", 1))\n",
    "            .withColumn(\"genres_spaced\", F.regexp_replace(col(\"genres\"), r\"\\|\", \" \"))\n",
    "            .withColumn(\"content\", concat_ws(\n",
    "                \" \", \n",
    "                col(\"title\"),\n",
    "                col(\"genres_spaced\"), col(\"genres_spaced\"), col(\"genres_spaced\"), col(\"genres_spaced\"),\n",
    "                col(\"year\"), col(\"decade\")\n",
    "            ))\n",
    "        )\n",
    "\n",
    "        stages = []\n",
    "\n",
    "        stages += [\n",
    "            Tokenizer(inputCol=\"content\", outputCol=\"raw_words\"),\n",
    "            StopWordsRemover(inputCol=\"raw_words\", outputCol=\"words\")\n",
    "        ]\n",
    "        \n",
    "        stages += [\n",
    "            HashingTF(inputCol=\"words\", outputCol=\"tf_uni\", numFeatures=self.num_features),\n",
    "            IDF(inputCol=\"tf_uni\", outputCol=\"tfidf_uni\", minDocFreq=1)\n",
    "        ]\n",
    "        \n",
    "        stages += [\n",
    "            NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\"),\n",
    "            HashingTF(inputCol=\"bigrams\", outputCol=\"tf_bi\", numFeatures=self.bigram_features),\n",
    "            IDF(inputCol=\"tf_bi\", outputCol=\"tfidf_bi\", minDocFreq=1)\n",
    "        ]\n",
    "        \n",
    "        pipeline = Pipeline(stages=stages)\n",
    "        self.vector_model = pipeline.fit(df)\n",
    "        features_df = self.vector_model.transform(df)\n",
    "        \n",
    "        combiner = self.combine_binarize_udf()\n",
    "        self.movies_binary = (\n",
    "            features_df\n",
    "            .withColumn(\"binary_features\", combiner(\"tfidf_uni\", \"tfidf_bi\"))\n",
    "            .select(\"item_id\", \"title\", \"genres\", \"binary_features\")\n",
    "            .cache()\n",
    "        )\n",
    "        \n",
    "        print(f\"checked {self.movies_binary.count()} movies\")\n",
    "\n",
    "    def build_lsh_index(self):\n",
    "        print(\"indexing with minhash LSH\")\n",
    "        \n",
    "        mh = MinHashLSH(\n",
    "            inputCol=\"binary_features\", \n",
    "            outputCol=\"hashes\", \n",
    "            numHashTables=self.num_hash_tables,\n",
    "            seed=42\n",
    "        )\n",
    "        self.lsh_model = mh.fit(self.movies_binary)\n",
    "        \n",
    "        dist_threshold = 1.0 - self.jaccard_threshold\n",
    "        \n",
    "        print(f\"cmputing similarity graph\")\n",
    "        raw_pairs = self.lsh_model.approxSimilarityJoin(\n",
    "            self.movies_binary, self.movies_binary, \n",
    "            threshold=dist_threshold, \n",
    "            distCol=\"jaccard_dist\"\n",
    "        )\n",
    "        \n",
    "        pairs = raw_pairs.select(\n",
    "            col(\"datasetA.item_id\").alias(\"item_a\"),\n",
    "            col(\"datasetB.item_id\").alias(\"item_b\"),\n",
    "            (1.0 - col(\"jaccard_dist\")).alias(\"similarity\")\n",
    "        ).filter(\"item_a != item_b\")\n",
    "        \n",
    "        self.similar_pairs = pairs.cache()\n",
    "        print(f\"indexed {self.similar_pairs.count()} similar item pairs\")\n",
    "\n",
    "    def recommend_for_users(self, train_df, items_df, k=10):\n",
    "        print(\"generating content-based recommendations\")\n",
    "        \n",
    "        user_history = train_df.filter(col(\"rating\") >= 4.0).select(\n",
    "            col(\"user_id\"), col(\"item_id\").alias(\"seed_item\"), col(\"rating\")\n",
    "        )\n",
    "\n",
    "        candidates = user_history.join(\n",
    "            self.similar_pairs,\n",
    "            user_history.seed_item == self.similar_pairs.item_a\n",
    "        )\n",
    "        \n",
    "        genre_df = items_df.select(\"item_id\", split(col(\"genres\"), r\"\\|\").alias(\"genres_arr\"))        \n",
    "        \n",
    "        candidates_enriched = (\n",
    "            candidates.alias(\"c\")\n",
    "            .join(\n",
    "                genre_df.alias(\"seed_g\"), \n",
    "                col(\"c.seed_item\") == col(\"seed_g.item_id\")\n",
    "            )\n",
    "            .join(\n",
    "                genre_df.alias(\"cand_g\"), \n",
    "                col(\"c.item_b\") == col(\"cand_g.item_id\")\n",
    "            )\n",
    "            .select(\n",
    "                \"c.user_id\", \n",
    "                col(\"c.item_b\").alias(\"candidate_item\"),\n",
    "                \"c.rating\", \n",
    "                \"c.similarity\",\n",
    "                size(array_intersect(\n",
    "                    col(\"seed_g.genres_arr\"), \n",
    "                    col(\"cand_g.genres_arr\")\n",
    "                )).alias(\"genre_overlap\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        filtered = candidates_enriched.filter(col(\"genre_overlap\") >= self.min_genre_overlap)\n",
    "\n",
    "        scored = filtered.withColumn(\n",
    "            \"score\", \n",
    "            col(\"rating\") * col(\"similarity\") * (1.0 + 0.25 * col(\"genre_overlap\"))\n",
    "        )\n",
    "\n",
    "        recs = scored.groupBy(\"user_id\", \"candidate_item\").agg(_sum(\"score\").alias(\"content_score\"))\n",
    "        \n",
    "        seen_items = train_df.select(\"user_id\", \"item_id\").distinct().alias(\"seen\")\n",
    "        recs_alias = recs.alias(\"recs\")\n",
    "        \n",
    "        final_recs = recs_alias.join(\n",
    "            seen_items,\n",
    "            (col(\"recs.user_id\") == col(\"seen.user_id\")) & \n",
    "            (col(\"recs.candidate_item\") == col(\"seen.item_id\")),\n",
    "            \"left_anti\"\n",
    "        ).select(\n",
    "            col(\"recs.user_id\"), \n",
    "            col(\"recs.candidate_item\").alias(\"item_id\"), \n",
    "            col(\"recs.content_score\")\n",
    "        )\n",
    "\n",
    "        window = Window.partitionBy(\"user_id\").orderBy(desc(\"content_score\"))\n",
    "        return (\n",
    "            final_recs\n",
    "            .withColumn(\"rank\", row_number().over(window))\n",
    "            .filter(col(\"rank\") <= k)\n",
    "            .drop(\"rank\")\n",
    "        )\n",
    "\n",
    "cb_filter = ContentBasedFilter()\n",
    "cb_filter.train_features(items_df)\n",
    "cb_filter.build_lsh_index()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building content features (tfidf + bigrams)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked 3883 movies\n",
      "indexing with minhash LSH\n",
      "cmputing similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 83323:==========================>                         (10 + 10) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexed 988212 similar item pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:43:18.414182Z",
     "start_time": "2026-01-18T14:43:18.279314Z"
    }
   },
   "source": [
    "content_recs = cb_filter.recommend_for_users(train_df, items_df, k=100)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating content-based recommendations\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:44:20.046870Z",
     "start_time": "2026-01-18T14:43:18.415572Z"
    }
   },
   "source": [
    "content_recs = content_recs.cache()\n",
    "content_recs.show(5)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 83341:==========================>                         (10 + 10) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+\n",
      "|user_id|item_id|     content_score|\n",
      "+-------+-------+------------------+\n",
      "|     95|   1744| 22.83967523704366|\n",
      "|     95|   1591|22.473956043956044|\n",
      "|     95|    849|21.873400389932648|\n",
      "|     95|   2334|21.532246786394385|\n",
      "|     95|   2058| 21.53224678639438|\n",
      "+-------+-------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUJnnDrCh518"
   },
   "source": "## 7.Fusion & Evaluation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9PM6sSWqh518",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:44:20.115722Z",
     "start_time": "2026-01-18T14:44:20.102851Z"
    }
   },
   "source": "ALPHA = 0.7",
   "outputs": [],
   "execution_count": 106
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-PgBqkXh518"
   },
   "source": "### 7.1 Normalization"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z3B5QAFnh518",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:44:20.123369Z",
     "start_time": "2026-01-18T14:44:20.116936Z"
    }
   },
   "source": [
    "def normalize(df, col_name):\n",
    "    stats = df.agg(F.min(col_name).alias(\"min\"), F.max(col_name).alias(\"max\")).collect()[0]\n",
    "    if stats[\"max\"] == stats[\"min\"]:\n",
    "        return df.withColumn(col_name + \"_norm\", F.lit(0.5))\n",
    "    return df.withColumn(col_name + \"_norm\", (F.col(col_name) - stats[\"min\"]) / (stats[\"max\"] - stats[\"min\"]))"
   ],
   "outputs": [],
   "execution_count": 107
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHZure3Wh518"
   },
   "source": "### 7.2 Hybrid Fusion"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "63j6mXT6h518",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:44:20.452317Z",
     "start_time": "2026-01-18T14:44:20.124463Z"
    }
   },
   "source": [
    "als_norm = normalize(als_recs, \"als_score\")\n",
    "content_norm = normalize(content_recs, \"content_score\")\n",
    "\n",
    "hybrid_recs = als_norm.select(\"user_id\", \"item_id\", \"als_score_norm\") \\\n",
    "    .join(content_norm.select(\"user_id\", \"item_id\", \"content_score_norm\"), [\"user_id\", \"item_id\"], \"full_outer\") \\\n",
    "    .fillna(0) \\\n",
    "    .withColumn(\"final_score\", ALPHA * F.col(\"als_score_norm\") + (1 - ALPHA) * F.col(\"content_score_norm\"))"
   ],
   "outputs": [],
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEQFjotih518",
    "outputId": "6edb4ce3-e385-4ee9-e167-5ea8474b4f57",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:44:21.679165Z",
     "start_time": "2026-01-18T14:44:20.453616Z"
    }
   },
   "source": [
    "hybrid_recs.orderBy(F.desc(\"final_score\")).show(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+--------------------+------------------+\n",
      "|user_id|item_id|    als_score_norm|  content_score_norm|       final_score|\n",
      "+-------+-------+------------------+--------------------+------------------+\n",
      "|   3902|   2964|               1.0|                 0.0|               0.7|\n",
      "|    283|   2964| 0.970525828139327|                 0.0|0.6793680796975289|\n",
      "|   4751|   2964|0.9128424254622094|0.043014639394282926|0.6518940896418314|\n",
      "|    101|   2964|0.9308630025862016|                 0.0|0.6516041018103411|\n",
      "|   5137|   2964|0.9289508859015769|                 0.0|0.6502656201311038|\n",
      "|   4169|   2493|0.5985305637891577|  0.7406185525735851|0.6411569604244859|\n",
      "|   4751|   2493|0.9050058385148974|                 0.0|0.6335040869604281|\n",
      "|    336|   2964|0.8836634842371193|                 0.0|0.6185644389659835|\n",
      "|   4751|   3181|0.8606336894297447| 0.05157650535458995|0.6179165342071983|\n",
      "|   4169|   2197|0.5614094295499126|  0.7420784076418976|0.6156101229775082|\n",
      "+-------+-------+------------------+--------------------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:44:22.232329Z",
     "start_time": "2026-01-18T14:44:21.702580Z"
    }
   },
   "source": [
    "hybrid_recs.orderBy(F.desc(\"final_score\")).filter((F.col(\"als_score_norm\") > 0.2) & (F.col(\"content_score_norm\") > 0.2)).show(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+-------------------+------------------+\n",
      "|user_id|item_id|    als_score_norm| content_score_norm|       final_score|\n",
      "+-------+-------+------------------+-------------------+------------------+\n",
      "|   4169|   2493|0.5985305637891577| 0.7406185525735851|0.6411569604244859|\n",
      "|   4169|   2197|0.5614094295499126| 0.7420784076418976|0.6156101229775082|\n",
      "|   4277|     53|0.6138623984164505|  0.590465080351217|0.6068432029968804|\n",
      "|   4169|     53|0.5492134331894858| 0.7100814164276091|0.5974738281609229|\n",
      "|   1812|   2305|0.7468649656254697|0.24769294234208108|0.5971133586404531|\n",
      "|   1448|   2197|0.6007690513598445|  0.533601971310933|0.5806189273451711|\n",
      "|   4277|   2579| 0.590152332050296| 0.5430159889973912|0.5760114291344246|\n",
      "|   1001|   2360|0.6525423504115732| 0.3470174152794822|0.5608848698719459|\n",
      "|   1448|     53|0.5767610465102394| 0.5233646290001703|0.5607421212572187|\n",
      "|    195|   2624|0.6270560862801566|0.37463481267083865|0.5513297041973612|\n",
      "+-------+-------+------------------+-------------------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:44:23.622869Z",
     "start_time": "2026-01-18T14:44:22.271891Z"
    }
   },
   "source": [
    "als_pairs = als_recs.select(\"user_id\", \"item_id\").distinct()\n",
    "content_pairs = content_recs.select(\"user_id\", \"item_id\").distinct()\n",
    "\n",
    "overlap = als_pairs.intersect(content_pairs).count()\n",
    "print(f\"als pairs: {als_pairs.count()}\")\n",
    "print(f\"content pairs: {content_pairs.count()}\")\n",
    "print(f\"overlap: {overlap}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "als pairs: 604000\n",
      "content pairs: 603761\n",
      "overlap: 12777\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcQId4lLh52C"
   },
   "source": "### 7.3 Evaluation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4_55bcPh52C",
    "outputId": "6b5486e9-a788-4a38-b856-37791abbc479",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:44:25.328085Z",
     "start_time": "2026-01-18T14:44:23.636719Z"
    }
   },
   "source": [
    "als_metrics = evaluate(als_recs, \"als_score\", \"ALS\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS: P@10=0.0257, R@10=0.0184, NDCG@10=0.0274\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "La8eg0aFh52D",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:44:26.268056Z",
     "start_time": "2026-01-18T14:44:25.344108Z"
    }
   },
   "source": [
    "content_metrics = evaluate(content_recs, \"content_score\", \"Content-Based\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content-Based: P@10=0.0150, R@10=0.0167, NDCG@10=0.0188\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9hpFNv0Vh52D",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:44:29.030812Z",
     "start_time": "2026-01-18T14:44:26.278989Z"
    }
   },
   "source": [
    "hybrid_metrics = evaluate(hybrid_recs, \"final_score\", \"Hybrid\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid: P@10=0.0270, R@10=0.0194, NDCG@10=0.0294\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kL3phVEuh52D"
   },
   "source": [
    "### Bonus: GBT Re-Ranking"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eZfDCdK1h52D"
   },
   "source": [
    "# TODO"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqAeFwu6h52D"
   },
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FlHvgvrWh52D",
    "ExecuteTime": {
     "end_time": "2026-01-18T14:45:31.474827Z",
     "start_time": "2026-01-18T14:45:30.316525Z"
    }
   },
   "source": [
    "summary = [\n",
    "    (\"ALS\", als_metrics[\"Precision@10\"], als_metrics[\"Recall@10\"], als_metrics[\"NDCG@10\"]),\n",
    "    (\"Content-Based\", content_metrics[\"Precision@10\"], content_metrics[\"Recall@10\"], content_metrics[\"NDCG@10\"]),\n",
    "    (\"Hybrid\", hybrid_metrics[\"Precision@10\"], hybrid_metrics[\"Recall@10\"], hybrid_metrics[\"NDCG@10\"]),\n",
    "]\n",
    "spark.createDataFrame(summary, [\"Model\", \"Precision@10\", \"Recall@10\", \"NDCG@10\"]).show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------------------+--------------------+\n",
      "|        Model|       Precision@10|           Recall@10|             NDCG@10|\n",
      "+-------------+-------------------+--------------------+--------------------+\n",
      "|          ALS|0.02574058577405817|  0.0184436548522113|0.027446874479665193|\n",
      "|Content-Based|0.01497907949790795|0.016735285093672912| 0.01879822686955161|\n",
      "|       Hybrid|0.02696234309623436| 0.01942285626524374| 0.02937548571130335|\n",
      "+-------------+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:45:35.393852Z",
     "start_time": "2026-01-18T14:45:34.590058Z"
    }
   },
   "source": [
    "# Sample User Recommendations Visualization\n",
    "import random\n",
    "\n",
    "# Pick a random user from test set\n",
    "test_users = test_df.select(\"user_id\").distinct().collect()\n",
    "sample_user_id = random.choice(test_users)[\"user_id\"]\n",
    "print(f\"Sample User ID: {sample_user_id}\\n\")\n",
    "\n",
    "# Get user's highly-rated movies from training set (rating >= 4)\n",
    "user_liked = train_df.filter(\n",
    "    (F.col(\"user_id\") == sample_user_id) & (F.col(\"rating\") >= 4.0)\n",
    ").join(items_df, \"item_id\").select(\"title\", \"genres\", \"rating\").orderBy(F.desc(\"rating\"))\n",
    "\n",
    "print(\"=== Movies rated highly by this user (training data) ===\")\n",
    "user_liked.show(10, truncate=False)\n",
    "\n",
    "# Get hybrid recommendations for this user\n",
    "user_recs = hybrid_recs.filter(F.col(\"user_id\") == sample_user_id) \\\n",
    "    .join(items_df, \"item_id\") \\\n",
    "    .select(\"title\", \"genres\", \"final_score\") \\\n",
    "    .orderBy(F.desc(\"final_score\"))\n",
    "\n",
    "print(\"=== Hybrid model recommendations ===\")\n",
    "user_recs.show(10, truncate=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample User ID: 1320\n",
      "\n",
      "=== Movies rated highly by this user (training data) ===\n",
      "+-----------------------------------------------------+---------------------------------+------+\n",
      "|title                                                |genres                           |rating|\n",
      "+-----------------------------------------------------+---------------------------------+------+\n",
      "|Out of Sight (1998)                                  |Action|Crime|Romance             |5.0   |\n",
      "|Glory (1989)                                         |Action|Drama|War                 |5.0   |\n",
      "|Limey, The (1999)                                    |Action|Crime|Drama               |5.0   |\n",
      "|Godfather, The (1972)                                |Action|Crime|Drama               |5.0   |\n",
      "|Alien (1979)                                         |Action|Horror|Sci-Fi|Thriller    |5.0   |\n",
      "|Hard-Boiled (Lashou shentan) (1992)                  |Action|Crime                     |5.0   |\n",
      "|Boat, The (Das Boot) (1981)                          |Action|Drama|War                 |5.0   |\n",
      "|Star Wars: Episode V - The Empire Strikes Back (1980)|Action|Adventure|Drama|Sci-Fi|War|5.0   |\n",
      "|Jaws (1975)                                          |Action|Horror                    |5.0   |\n",
      "|Killer, The (Die xue shuang xiong) (1989)            |Action|Thriller                  |5.0   |\n",
      "+-----------------------------------------------------+---------------------------------+------+\n",
      "only showing top 10 rows\n",
      "=== Hybrid model recommendations ===\n",
      "+-----------------------------------------+--------------------------+-------------------+\n",
      "|title                                    |genres                    |final_score        |\n",
      "+-----------------------------------------+--------------------------+-------------------+\n",
      "|Cabaret Balkan (Bure Baruta) (1998)      |Drama                     |0.43502790954351456|\n",
      "|Arguing the World (1996)                 |Documentary               |0.4344735829977842 |\n",
      "|Paths of Glory (1957)                    |Drama|War                 |0.42575815650318904|\n",
      "|Killer, The (Die xue shuang xiong) (1989)|Action|Thriller           |0.4249329323436876 |\n",
      "|Almost Famous (2000)                     |Comedy|Drama              |0.41975618161708644|\n",
      "|When We Were Kings (1996)                |Documentary               |0.4186839114704492 |\n",
      "|Fargo (1996)                             |Crime|Drama|Thriller      |0.4179040738499314 |\n",
      "|Chinatown (1974)                         |Film-Noir|Mystery|Thriller|0.4170019242455865 |\n",
      "|Godfather, The (1972)                    |Action|Crime|Drama        |0.41673860860584516|\n",
      "|Godfather: Part II, The (1974)           |Action|Crime|Drama        |0.4138905243057271 |\n",
      "+-----------------------------------------+--------------------------+-------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import Evaluator\n",
    "from pyspark.ml import Estimator, Model\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "import time\n",
    "\n",
    "class ContentBasedEstimator(Estimator, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    numFeatures = Param(Params._dummy(), \"numFeatures\", \"num of tf-idf features\", TypeConverters.toInt)\n",
    "    bigramFeatures = Param(Params._dummy(), \"bigramFeatures\", \"num of bigram features\", TypeConverters.toInt)\n",
    "    numHashTables = Param(Params._dummy(), \"numHashTables\", \"num of LSH hash tables\", TypeConverters.toInt)\n",
    "    jaccardThreshold = Param(Params._dummy(), \"jaccardThreshold\", \"jaccard similarity threshold\", TypeConverters.toFloat)\n",
    "    minGenreOverlap = Param(Params._dummy(), \"minGenreOverlap\", \"minimum genre overlap\", TypeConverters.toInt)\n",
    "    \n",
    "    def __init__(self, numFeatures=5000, bigramFeatures=3000, \n",
    "                numHashTables=10, jaccardThreshold=0.1, minGenreOverlap=1):\n",
    "        super(ContentBasedEstimator, self).__init__()\n",
    "        self._setDefault(numFeatures=numFeatures, \n",
    "                        bigramFeatures=bigramFeatures, \n",
    "                        numHashTables=numHashTables,\n",
    "                        jaccardThreshold=jaccardThreshold,\n",
    "                        minGenreOverlap=minGenreOverlap)\n",
    "        \n",
    "        self._set(numFeatures=numFeatures, \n",
    "                bigramFeatures=bigramFeatures, \n",
    "                numHashTables=numHashTables,\n",
    "                jaccardThreshold=jaccardThreshold, \n",
    "                minGenreOverlap=minGenreOverlap)\n",
    "    \n",
    "    def _fit(self, dataset):\n",
    "        cb = ContentBasedFilter(\n",
    "            num_features=self.getOrDefault(self.numFeatures),\n",
    "            bigram_features=self.getOrDefault(self.bigramFeatures),\n",
    "            num_hash_tables=self.getOrDefault(self.numHashTables)\n",
    "        )\n",
    "        cb.jaccard_threshold = self.getOrDefault(self.jaccardThreshold)\n",
    "        cb.min_genre_overlap = self.getOrDefault(self.minGenreOverlap)\n",
    "        cb.train_features(items_df)\n",
    "        cb.build_lsh_index()\n",
    "        return ContentBasedModel(cb)\n",
    "\n",
    "class ContentBasedModel(Model, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self, cb_filter=None):\n",
    "        super(ContentBasedModel, self).__init__()\n",
    "        self.cb_filter = cb_filter\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        if self.cb_filter is None:\n",
    "            raise ValueError(\"model not fitted\")\n",
    "        return self.cb_filter.recommend_for_users(dataset, items_df, k=100)\n",
    "\n",
    "class RecSysEvaluator(Evaluator):\n",
    "    def __init__(self, test_df, ground_truth, k=10):\n",
    "        super(RecSysEvaluator, self).__init__()\n",
    "        self.test_df = test_df\n",
    "        self.ground_truth = ground_truth\n",
    "        self.k = k\n",
    "    \n",
    "    def _evaluate(self, dataset):\n",
    "        top_k = get_top_k(dataset, \"content_score\", self.k)\n",
    "        return ndcg_at_k(top_k, self.ground_truth, self.k)\n",
    "    \n",
    "    def isLargerBetter(self):\n",
    "        return True\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(ContentBasedEstimator.numFeatures, [3000, 5000, 7000]) \\\n",
    "    .addGrid(ContentBasedEstimator.bigramFeatures, [2000, 3000]) \\\n",
    "    .addGrid(ContentBasedEstimator.numHashTables, [10, 20, 30]) \\\n",
    "    .addGrid(ContentBasedEstimator.jaccardThreshold, [0.1, 0.2]) \\\n",
    "    .addGrid(ContentBasedEstimator.minGenreOverlap, [1, 2]) \\\n",
    "    .build()\n",
    "\n",
    "estimator = ContentBasedEstimator()\n",
    "evaluator = RecSysEvaluator(test_df, ground_truth, k=10)\n",
    "items_df.cache()\n",
    "train_df.cache()\n",
    "cv = CrossValidator(estimator=estimator, estimatorParamMaps=paramGrid, \n",
    "                    evaluator=evaluator, numFolds=2, parallelism=10)\n",
    "\n",
    "start = time.time()\n",
    "cvModel = cv.fit(train_df)\n",
    "train_time = time.time() - start\n",
    "\n",
    "best_model = cvModel.bestModel\n",
    "print(f\"Training time: {train_time:.2f}s\")\n",
    "print(f\"Best params: numFeatures={best_model.cb_filter.num_features}, bigramFeatures={best_model.cb_filter.bigram_features}, numHashTables={best_model.cb_filter.num_hash_tables}, jaccardThreshold={best_model.cb_filter.jaccard_threshold:.3f}, minGenreOverlap={best_model.cb_filter.min_genre_overlap}\")\n",
    "\n",
    "start = time.time()\n",
    "tuned_recs = best_model.transform(train_df).cache()\n",
    "inference_time = time.time() - start\n",
    "\n",
    "tuned_metrics = evaluate(tuned_recs, \"content_score\", \"Tuned Content-Based\")\n",
    "print(f\"Inference time: {inference_time:.2f}s\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "26/01/17 19:33:09 WARN CacheManager: Asked to cache already cached data.\n",
    "26/01/17 19:33:09 WARN CacheManager: Asked to cache already cached data.\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairsindexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "building content features (tfidf + bigrams)\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "building content features (tfidf + bigrams)\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "checked 3883 moviesgenerating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "\n",
    "indexing with minhash LSH\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "building content features (tfidf + bigrams)\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendationsgenerating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "\n",
    "generating content-based recommendations\n",
    "indexing with minhash LSH\n",
    "generating content-based recommendations\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "building content features (tfidf + bigrams)\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "indexed 988212 similar item pairsindexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "building content features (tfidf + bigrams)\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 moviesindexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "checked 3883 movies\n",
    "indexed 988212 similar item pairs\n",
    "\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "indexing with minhash LSH\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graphcmputing similarity graph\n",
    "\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "indexed 988212 similar item pairsindexed 988212 similar item pairs\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "generating content-based recommendations\n",
    "\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "...\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n",
    "                                                                                \n",
    "indexed 988212 similar item pairsindexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "building content features (tfidf + bigrams)\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "cmputing similarity graph\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "indexed 988212 similar item pairsindexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "...\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movieschecked 3883 movies\n",
    "indexing with minhash LSH\n",
    "\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graphcmputing similarity graph\n",
    "\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "indexed 988212 similar item pairsindexed 988212 similar item pairs\n",
    "\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "checked 3883 movieschecked 3883 movies\n",
    "indexing with minhash LSH\n",
    "\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "Training time: 5721.41s\n",
    "Best params: numFeatures=5000, bigramFeatures=3000, numHashTables=10, jaccardThreshold=0.100, minGenreOverlap=1\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "Tuned Content-Based: P@10=0.0150, R@10=0.0167, NDCG@10=0.0188\n",
    "Inference time: 0.09s\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "history_visible": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mining-massive-databases",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
