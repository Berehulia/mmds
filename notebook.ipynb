{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jV-qaRjjh51v"
   },
   "source": [
    "# Hybrid Recommendation System\n",
    "\n",
    "**Team Structure:**\n",
    "- Member 1: Infrastructure, Data Loading, Fusion & Evaluation\n",
    "- Member 2: Collaborative Filtering (ALS)\n",
    "- Member 3: Content-Based Filtering (TF-IDF + LSH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z3aeJVqkPIa"
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnzLbX57kUnJ"
   },
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GMg3a2fqh51y"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from math import log2\n",
    "\n",
    "# Fix for Windows\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, FloatType, StructType, StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d--B_svch51z"
   },
   "source": [
    "### 1.2 Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7xDJxcgHh511"
   },
   "outputs": [],
   "source": [
    "DATA_URL = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
    "DATA_DIR = \"data\"\n",
    "DATASET_DIR = os.path.join(DATA_DIR, \"ml-1m\")\n",
    "ZIP_PATH = os.path.join(DATA_DIR, \"ml-1m.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2xaxdp_xh512"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3cKFv2Bh513"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DATASET_DIR):\n",
    "\n",
    "    if not os.path.exists(ZIP_PATH):\n",
    "        print(\"Downloading MovieLens ml-1m...\")\n",
    "        urllib.request.urlretrieve(DATA_URL, ZIP_PATH)\n",
    "\n",
    "    print(\"Extracting...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew2EJ0KCh513"
   },
   "source": [
    "### 1.3 Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zH6tSX_bh513"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/15 12:31:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"MMDS\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y95TFnL2h514"
   },
   "source": [
    "### 1.4 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBLTLe1th514",
    "outputId": "fc2b2efd-dfe3-4498-e34a-ba1de16a3140"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6040"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df = spark.read.text(os.path.join(DATASET_DIR, \"users.dat\")).select(\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(0).cast(IntegerType()).alias(\"user_id\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(1).alias(\"gender\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(2).cast(IntegerType()).alias(\"age\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(3).cast(IntegerType()).alias(\"occupation\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(4).alias(\"zip_code\")\n",
    ")\n",
    "users_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFLr1EX2h514",
    "outputId": "08e9e755-c1b5-4160-d625-643f128fc28e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_df = spark.read.text(os.path.join(DATASET_DIR, \"movies.dat\")).select(\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(0).cast(IntegerType()).alias(\"item_id\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(1).alias(\"title\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(2).alias(\"genres\")\n",
    ")\n",
    "items_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uafyQhs3h515",
    "outputId": "1af79a1e-18be-442c-d72f-fa38cf29fbed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000209"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df = spark.read.text(os.path.join(DATASET_DIR, \"ratings.dat\")).select(\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(0).cast(IntegerType()).alias(\"user_id\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(1).cast(IntegerType()).alias(\"item_id\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(2).cast(FloatType()).alias(\"rating\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(3).cast(IntegerType()).alias(\"timestamp\")\n",
    ")\n",
    "ratings_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36xJVL9Ih515",
    "outputId": "30aaf041-8590-4453-c239-224def2157c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+----------+--------+\n",
      "|user_id|gender|age|occupation|zip_code|\n",
      "+-------+------+---+----------+--------+\n",
      "|      1|     F|  1|        10|   48067|\n",
      "|      2|     M| 56|        16|   70072|\n",
      "|      3|     M| 25|        15|   55117|\n",
      "|      4|     M| 45|         7|   02460|\n",
      "|      5|     M| 25|        20|   55455|\n",
      "+-------+------+---+----------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "users_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtXD-ucvh515",
    "outputId": "fe9aaea5-5745-46bc-aa52-f8438ba62fe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|item_id|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Animation|Childre...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|        Comedy|Drama|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "items_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BI5_IghMh515",
    "outputId": "e390641c-b757-445d-af40-8b20d25ed3a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+---------+\n",
      "|user_id|item_id|rating|timestamp|\n",
      "+-------+-------+------+---------+\n",
      "|      1|   1193|   5.0|978300760|\n",
      "|      1|    661|   3.0|978302109|\n",
      "|      1|    914|   3.0|978301968|\n",
      "|      1|   3408|   4.0|978300275|\n",
      "|      1|   2355|   5.0|978824291|\n",
      "+-------+-------+------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYsOYCQEh515"
   },
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmGEbwy3mDDY"
   },
   "source": [
    "### 2.1 Rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cARZQAvGh515",
    "outputId": "aa9e00a1-9cec-4884-c3e8-bce725927b73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users:            6,040\n",
      "Movies:           3,883\n",
      "Ratings:          1,000,209\n",
      "Sparsity:         95.74%\n",
      "Avg ratings/user: 165.6\n",
      "Avg ratings/movie:257.6\n"
     ]
    }
   ],
   "source": [
    "num_users = users_df.count()\n",
    "num_items = items_df.count()\n",
    "num_ratings = ratings_df.count()\n",
    "sparsity = (1 - (num_ratings / (num_users * num_items))) * 100\n",
    "\n",
    "print(f\"Users:            {num_users:,}\")\n",
    "print(f\"Movies:           {num_items:,}\")\n",
    "print(f\"Ratings:          {num_ratings:,}\")\n",
    "print(f\"Sparsity:         {sparsity:.2f}%\")\n",
    "print(f\"Avg ratings/user: {num_ratings/num_users:.1f}\")\n",
    "print(f\"Avg ratings/movie:{num_ratings/num_items:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQaH_GJVmLpj"
   },
   "source": [
    "### 2.2 Rating distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlDMVOJNh515",
    "outputId": "dd79f2eb-46dd-4966-cbbc-0daebd89857e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|rating| count|\n",
      "+------+------+\n",
      "|   1.0| 56174|\n",
      "|   2.0|107557|\n",
      "|   3.0|261197|\n",
      "|   4.0|348971|\n",
      "|   5.0|226310|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratings_df.groupBy(\"rating\").count().orderBy(\"rating\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMl6TOXvmQiN"
   },
   "source": [
    "### 2.3 Genre distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4uolq8hkh515",
    "outputId": "8a63ec2b-7dc1-4c5a-9a12-390dbc0b070c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|      genre|count|\n",
      "+-----------+-----+\n",
      "|      Drama| 1603|\n",
      "|     Comedy| 1200|\n",
      "|     Action|  503|\n",
      "|   Thriller|  492|\n",
      "|    Romance|  471|\n",
      "|     Horror|  343|\n",
      "|  Adventure|  283|\n",
      "|     Sci-Fi|  276|\n",
      "| Children's|  251|\n",
      "|      Crime|  211|\n",
      "|        War|  143|\n",
      "|Documentary|  127|\n",
      "|    Musical|  114|\n",
      "|    Mystery|  106|\n",
      "|  Animation|  105|\n",
      "|    Fantasy|   68|\n",
      "|    Western|   68|\n",
      "|  Film-Noir|   44|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_df.select(F.explode(F.split(F.col(\"genres\"), \"\\\\|\")).alias(\"genre\")) \\\n",
    "    .groupBy(\"genre\").count().orderBy(F.desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M4BhkBOmoPr"
   },
   "source": [
    "### 2.4 User gender distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWtkOG1rh515",
    "outputId": "1955049f-0ef8-40c7-8568-765dfc3d126c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     F| 1709|\n",
      "|     M| 4331|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.groupBy(\"gender\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_t-YXlxmtrw"
   },
   "source": [
    "### 2.5 User age distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tt5JfAqKh515",
    "outputId": "5e18a191-b997-4b90-fcb5-9576ecaafb17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "|  1|  222|\n",
      "| 18| 1103|\n",
      "| 25| 2096|\n",
      "| 35| 1193|\n",
      "| 45|  550|\n",
      "| 50|  496|\n",
      "| 56|  380|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.groupBy(\"age\").count().orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWmkYV91h516"
   },
   "source": [
    "## 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "r2lL2tL-h516"
   },
   "outputs": [],
   "source": [
    "train_df, test_df = ratings_df.randomSplit([0.8, 0.2], seed=42)\n",
    "train_df = train_df.cache()\n",
    "test_df = test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7bz6lAKh516",
    "outputId": "43cfc1af-6a46-4cdf-f794-ca94e38ad55a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "800053"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ambqvc_Fh516",
    "outputId": "a85471a3-77c0-4640-d912-043c97d3bca4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200156"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZBXPcymh517"
   },
   "source": [
    "## 4. Collaborative Filtering (ALS)\n",
    "\n",
    "Implement using `pyspark.ml.recommendation.ALS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tR9qfStQh517"
   },
   "outputs": [],
   "source": [
    "class CollaborativeFilter:\n",
    "\n",
    "    def __init__(self, rank=10, regParam=0.1, maxIter=10):\n",
    "        self.als = ALS(\n",
    "            userCol=\"user_id\",\n",
    "            itemCol=\"item_id\",\n",
    "            ratingCol=\"rating\",\n",
    "            rank=rank,\n",
    "            regParam=regParam,\n",
    "            maxIter=maxIter,\n",
    "            coldStartStrategy=\"drop\",\n",
    "            nonnegative=True\n",
    "        )\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, df):\n",
    "        self.model = self.als.fit(df)\n",
    "\n",
    "    def get_recommendations(self, df, k=10):\n",
    "        \"\"\"Get top-K recommendations\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Call train() first.\")\n",
    "\n",
    "        users = df.select(\"user_id\").distinct()\n",
    "        user_recs = self.model.recommendForUserSubset(users, k)\n",
    "\n",
    "        return user_recs.select(\n",
    "            F.col(\"user_id\"),\n",
    "            F.explode(\"recommendations\").alias(\"rec\")\n",
    "        ).select(\n",
    "            F.col(\"user_id\"),\n",
    "            F.col(\"rec.item_id\").cast(IntegerType()).alias(\"item_id\"),\n",
    "            F.col(\"rec.rating\").alias(\"prediction\")\n",
    "        )\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"Predict ratings for user-item pairs in test\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Train should be called first\")\n",
    "        return self.model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "FxOIEA61GHB5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 12:31:20 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cf = CollaborativeFilter(rank=10, regParam=0.1, maxIter=10)\n",
    "cf.train(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "tnm-lWUrGgMU"
   },
   "outputs": [],
   "source": [
    "als_recs = cf.get_recommendations(test_df, k=100).withColumnRenamed(\"prediction\", \"als_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RzSY2W66h517",
    "outputId": "b0348579-c8ab-443b-f6b3-6f9492bb92c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 121:=====================================================>(99 + 1) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+\n",
      "|user_id|item_id|als_score|\n",
      "+-------+-------+---------+\n",
      "|      1|   3233| 4.569002|\n",
      "|      1|    128|4.4839425|\n",
      "|      1|    527| 4.477343|\n",
      "|      1|    318| 4.457619|\n",
      "|      1|   1851|4.4368587|\n",
      "|      1|   3172|4.4209924|\n",
      "|      1|    953|4.4157286|\n",
      "|      1|    919| 4.412204|\n",
      "|      1|    858| 4.402227|\n",
      "|      1|   1207|4.3944917|\n",
      "+-------+-------+---------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "als_recs.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxfposUDh517"
   },
   "source": [
    "### Bonus: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "bmVnX8K7h517"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMWQPlANh517"
   },
   "source": [
    "## 5. Content-Based Filtering (TF-IDF + LSH)\n",
    "\n",
    "Implement using `pyspark.ml.feature` (Tokenizer, HashingTF, IDF, BucketedRandomProjectionLSH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building content features (tfidf + bigrams)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked 3883 movies\n",
      "indexing with minhash LSH\n",
      "cmputing similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 157:=================================================>   (188 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexed 988212 similar item pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, NGram, MinHashLSH\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import (\n",
    "    col, split, concat_ws, regexp_extract, udf, array_intersect, \n",
    "    size, sum as _sum, desc, row_number\n",
    ")\n",
    "\n",
    "class ContentBasedFilter:\n",
    "    def __init__(self, num_features=5000, bigram_features=3000, num_hash_tables=10):\n",
    "        self.num_features = num_features\n",
    "        self.bigram_features = bigram_features\n",
    "        self.num_hash_tables = num_hash_tables\n",
    "        self.jaccard_threshold = 0.1\n",
    "        self.genre_weight = 4\n",
    "        self.min_genre_overlap = 1\n",
    "        \n",
    "\n",
    "        self.lsh_model = None\n",
    "        self.vector_model = None\n",
    "        self.movies_binary = None\n",
    "        self.similar_pairs = None\n",
    "\n",
    "    def combine_binarize_udf(self):\n",
    "        def process_vectors(v1, v2):\n",
    "            if v1 is None: \n",
    "                return None\n",
    "            indices = [int(i) for i in v1.indices]\n",
    "            \n",
    "            if v2 is not None:\n",
    "                offset = int(v1.size)\n",
    "                indices += [int(i) + offset for i in v2.indices]\n",
    "                total_size = offset + int(v2.size)\n",
    "            else:\n",
    "                total_size = int(v1.size)\n",
    "            \n",
    "            values = [1.0] * len(indices)\n",
    "            return Vectors.sparse(total_size, sorted(indices), values)\n",
    "            \n",
    "        return udf(process_vectors, VectorUDT())\n",
    "\n",
    "    def train_features(self, items_df):\n",
    "        print(\"building content features (tfidf + bigrams)\")\n",
    "        \n",
    "        df = (\n",
    "            items_df\n",
    "            .withColumn(\"year\", regexp_extract(col(\"title\"), r\"\\((\\d{4})\\)\", 1))\n",
    "            .withColumn(\"decade\", regexp_extract(col(\"title\"), r\"\\((\\d{3})\\d\\)\", 1))\n",
    "            .withColumn(\"genres_spaced\", F.regexp_replace(col(\"genres\"), r\"\\|\", \" \"))\n",
    "            .withColumn(\"content\", concat_ws(\n",
    "                \" \", \n",
    "                col(\"title\"),\n",
    "                col(\"genres_spaced\"), col(\"genres_spaced\"), col(\"genres_spaced\"), col(\"genres_spaced\"),\n",
    "                col(\"year\"), col(\"decade\")\n",
    "            ))\n",
    "        )\n",
    "\n",
    "        stages = []\n",
    "\n",
    "        stages += [\n",
    "            Tokenizer(inputCol=\"content\", outputCol=\"raw_words\"),\n",
    "            StopWordsRemover(inputCol=\"raw_words\", outputCol=\"words\")\n",
    "        ]\n",
    "        \n",
    "        stages += [\n",
    "            HashingTF(inputCol=\"words\", outputCol=\"tf_uni\", numFeatures=self.num_features),\n",
    "            IDF(inputCol=\"tf_uni\", outputCol=\"tfidf_uni\", minDocFreq=1)\n",
    "        ]\n",
    "        \n",
    "        stages += [\n",
    "            NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\"),\n",
    "            HashingTF(inputCol=\"bigrams\", outputCol=\"tf_bi\", numFeatures=self.bigram_features),\n",
    "            IDF(inputCol=\"tf_bi\", outputCol=\"tfidf_bi\", minDocFreq=1)\n",
    "        ]\n",
    "        \n",
    "        pipeline = Pipeline(stages=stages)\n",
    "        self.vector_model = pipeline.fit(df)\n",
    "        features_df = self.vector_model.transform(df)\n",
    "        \n",
    "        combiner = self.combine_binarize_udf()\n",
    "        self.movies_binary = (\n",
    "            features_df\n",
    "            .withColumn(\"binary_features\", combiner(\"tfidf_uni\", \"tfidf_bi\"))\n",
    "            .select(\"item_id\", \"title\", \"genres\", \"binary_features\")\n",
    "            .cache()\n",
    "        )\n",
    "        \n",
    "        print(f\"checked {self.movies_binary.count()} movies\")\n",
    "\n",
    "    def build_lsh_index(self):\n",
    "        print(\"indexing with minhash LSH\")\n",
    "        \n",
    "        mh = MinHashLSH(\n",
    "            inputCol=\"binary_features\", \n",
    "            outputCol=\"hashes\", \n",
    "            numHashTables=self.num_hash_tables,\n",
    "            seed=42\n",
    "        )\n",
    "        self.lsh_model = mh.fit(self.movies_binary)\n",
    "        \n",
    "        dist_threshold = 1.0 - self.jaccard_threshold\n",
    "        \n",
    "        print(f\"cmputing similarity graph\")\n",
    "        raw_pairs = self.lsh_model.approxSimilarityJoin(\n",
    "            self.movies_binary, self.movies_binary, \n",
    "            threshold=dist_threshold, \n",
    "            distCol=\"jaccard_dist\"\n",
    "        )\n",
    "        \n",
    "        pairs = raw_pairs.select(\n",
    "            col(\"datasetA.item_id\").alias(\"item_a\"),\n",
    "            col(\"datasetB.item_id\").alias(\"item_b\"),\n",
    "            (1.0 - col(\"jaccard_dist\")).alias(\"similarity\")\n",
    "        ).filter(\"item_a != item_b\")\n",
    "        \n",
    "        self.similar_pairs = pairs.cache()\n",
    "        print(f\"indexed {self.similar_pairs.count()} similar item pairs\")\n",
    "\n",
    "    def recommend_for_users(self, train_df, items_df, k=10):\n",
    "        print(\"generating content-based recommendations\")\n",
    "        \n",
    "        user_history = train_df.filter(col(\"rating\") >= 4.0).select(\n",
    "            col(\"user_id\"), col(\"item_id\").alias(\"seed_item\"), col(\"rating\")\n",
    "        )\n",
    "\n",
    "        candidates = user_history.join(\n",
    "            self.similar_pairs,\n",
    "            user_history.seed_item == self.similar_pairs.item_a\n",
    "        )\n",
    "        \n",
    "        genre_df = items_df.select(\"item_id\", split(col(\"genres\"), r\"\\|\").alias(\"genres_arr\"))        \n",
    "        \n",
    "        candidates_enriched = (\n",
    "            candidates.alias(\"c\")\n",
    "            .join(\n",
    "                genre_df.alias(\"seed_g\"), \n",
    "                col(\"c.seed_item\") == col(\"seed_g.item_id\")\n",
    "            )\n",
    "            .join(\n",
    "                genre_df.alias(\"cand_g\"), \n",
    "                col(\"c.item_b\") == col(\"cand_g.item_id\")\n",
    "            )\n",
    "            .select(\n",
    "                \"c.user_id\", \n",
    "                col(\"c.item_b\").alias(\"candidate_item\"),\n",
    "                \"c.rating\", \n",
    "                \"c.similarity\",\n",
    "                size(array_intersect(\n",
    "                    col(\"seed_g.genres_arr\"), \n",
    "                    col(\"cand_g.genres_arr\")\n",
    "                )).alias(\"genre_overlap\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        filtered = candidates_enriched.filter(col(\"genre_overlap\") >= self.min_genre_overlap)\n",
    "\n",
    "        scored = filtered.withColumn(\n",
    "            \"score\", \n",
    "            col(\"rating\") * col(\"similarity\") * (1.0 + 0.25 * col(\"genre_overlap\"))\n",
    "        )\n",
    "\n",
    "        recs = scored.groupBy(\"user_id\", \"candidate_item\").agg(_sum(\"score\").alias(\"content_score\"))\n",
    "        \n",
    "        seen_items = train_df.select(\"user_id\", \"item_id\").distinct().alias(\"seen\")\n",
    "        recs_alias = recs.alias(\"recs\")\n",
    "        \n",
    "        final_recs = recs_alias.join(\n",
    "            seen_items,\n",
    "            (col(\"recs.user_id\") == col(\"seen.user_id\")) & \n",
    "            (col(\"recs.candidate_item\") == col(\"seen.item_id\")),\n",
    "            \"left_anti\"\n",
    "        ).select(\n",
    "            col(\"recs.user_id\"), \n",
    "            col(\"recs.candidate_item\").alias(\"item_id\"), \n",
    "            col(\"recs.content_score\")\n",
    "        )\n",
    "\n",
    "        window = Window.partitionBy(\"user_id\").orderBy(desc(\"content_score\"))\n",
    "        return (\n",
    "            final_recs\n",
    "            .withColumn(\"rank\", row_number().over(window))\n",
    "            .filter(col(\"rank\") <= k)\n",
    "            .drop(\"rank\")\n",
    "        )\n",
    "\n",
    "cb_filter = ContentBasedFilter()\n",
    "cb_filter.train_features(items_df)\n",
    "cb_filter.build_lsh_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating content-based recommendations\n"
     ]
    }
   ],
   "source": [
    "content_recs = cb_filter.recommend_for_users(train_df, items_df, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 184:==============================================>      (176 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------------+\n",
      "|user_id|item_id|    content_score|\n",
      "+-------+-------+-----------------+\n",
      "|    148|   1591| 89.0484413983273|\n",
      "|    148|   1744|85.95443869216115|\n",
      "|    148|   1626|81.63936115826036|\n",
      "|    148|   2058|73.81954842988361|\n",
      "|    148|   2916|72.60978868408795|\n",
      "+-------+-------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "content_recs = content_recs.cache()\n",
    "content_recs.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUJnnDrCh518"
   },
   "source": [
    "## 6.Fusion & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9PM6sSWqh518"
   },
   "outputs": [],
   "source": [
    "ALPHA = 0.7\n",
    "K = 10\n",
    "RELEVANCE_THRESHOLD = 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-PgBqkXh518"
   },
   "source": [
    "### 6.1 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "z3B5QAFnh518"
   },
   "outputs": [],
   "source": [
    "def normalize(df, col_name):\n",
    "    stats = df.agg(F.min(col_name).alias(\"min\"), F.max(col_name).alias(\"max\")).collect()[0]\n",
    "    if stats[\"max\"] == stats[\"min\"]:\n",
    "        return df.withColumn(col_name + \"_norm\", F.lit(0.5))\n",
    "    return df.withColumn(col_name + \"_norm\", (F.col(col_name) - stats[\"min\"]) / (stats[\"max\"] - stats[\"min\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHZure3Wh518"
   },
   "source": [
    "### 6.2 Hybrid Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "63j6mXT6h518"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "als_norm = normalize(als_recs, \"als_score\")\n",
    "content_norm = normalize(content_recs, \"content_score\")\n",
    "\n",
    "hybrid_recs = als_norm.select(\"user_id\", \"item_id\", \"als_score_norm\") \\\n",
    "    .join(content_norm.select(\"user_id\", \"item_id\", \"content_score_norm\"), [\"user_id\", \"item_id\"], \"full_outer\") \\\n",
    "    .fillna(0) \\\n",
    "    .withColumn(\"final_score\", ALPHA * F.col(\"als_score_norm\") + (1 - ALPHA) * F.col(\"content_score_norm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEQFjotih518",
    "outputId": "6edb4ce3-e385-4ee9-e167-5ea8474b4f57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 343:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+------------------+------------------+\n",
      "|user_id|item_id|    als_score_norm|content_score_norm|       final_score|\n",
      "+-------+-------+------------------+------------------+------------------+\n",
      "|   4028|   1796|               1.0|               0.0|               0.7|\n",
      "|   4277|     53|0.6681412660360792|0.7317264247269524|0.6872168136433411|\n",
      "|   4169|     53|0.6548254695380753|0.7582726855607065|0.6858596343448646|\n",
      "|   2155|    108|0.9795053197530404|               0.0|0.6856537238271283|\n",
      "|   5328|   1796|0.9702602974655985|               0.0|0.6791822082259189|\n",
      "|   1445|   1138|0.9614800392212809|               0.0|0.6730360274548965|\n",
      "|   4110|   2192|0.9608376664849905|               0.0|0.6725863665394933|\n",
      "|   1445|   2962|0.9584632469582222|               0.0|0.6709242728707555|\n",
      "|   4703|   1796|0.9517324311833929|               0.0| 0.666212701828375|\n",
      "|   4277|   2624|0.6255933786116243| 0.754186712875991|0.6641713788909344|\n",
      "+-------+-------+------------------+------------------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hybrid_recs.orderBy(F.desc(\"final_score\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+-------------------+------------------+\n",
      "|user_id|item_id|    als_score_norm| content_score_norm|       final_score|\n",
      "+-------+-------+------------------+-------------------+------------------+\n",
      "|   4277|     53|0.6681412660360792| 0.7317264247269524|0.6872168136433411|\n",
      "|   4169|     53|0.6548254695380753| 0.7582726855607065|0.6858596343448646|\n",
      "|   4277|   2624|0.6255933786116243|  0.754186712875991|0.6641713788909344|\n",
      "|     53|     53|0.7598887207001292|0.35001256829402444|0.6369258749782978|\n",
      "|   1698|     53|0.7094904438537074| 0.4297322213632261| 0.625562977106563|\n",
      "|   4169|   2503|0.6107187996310084| 0.6442511305292286|0.6207784989004744|\n",
      "|    195|     53|0.6920743994723191| 0.4047206792058041|0.6058682833923645|\n",
      "|   1680|     53|0.6438432894186257|  0.513718058524496|0.6048057201503868|\n",
      "|   3902|    733| 0.766619280922882|0.20011356009613945|0.5966675646748592|\n",
      "|   1448|     53|0.6440464533194415| 0.4829970743570718|0.5957316396307306|\n",
      "+-------+-------+------------------+-------------------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "hybrid_recs.orderBy(F.desc(\"final_score\")).filter((F.col(\"als_score_norm\") > 0.2) & (F.col(\"content_score_norm\") > 0.2)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "als pairs: 603800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 659:==================================================>  (189 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content pairs: 603709\n",
      "overlap: 11312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "als_pairs = als_recs.select(\"user_id\", \"item_id\").distinct()\n",
    "content_pairs = content_recs.select(\"user_id\", \"item_id\").distinct()\n",
    "\n",
    "overlap = als_pairs.intersect(content_pairs).count()\n",
    "print(f\"als pairs: {als_pairs.count()}\")\n",
    "print(f\"content pairs: {content_pairs.count()}\")\n",
    "print(f\"overlap: {overlap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pO0RqOEah518"
   },
   "source": [
    "### 6.3 Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-6zhUdyvh518"
   },
   "outputs": [],
   "source": [
    "ground_truth = test_df.filter(F.col(\"rating\") >= RELEVANCE_THRESHOLD) \\\n",
    "    .groupBy(\"user_id\").agg(F.collect_list(\"item_id\").alias(\"relevant_items\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LPsv7vPh518"
   },
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Qi9mYfoYh518"
   },
   "outputs": [],
   "source": [
    "def get_top_k(recs_df, score_col, k):\n",
    "    window = Window.partitionBy(\"user_id\").orderBy(F.desc(score_col))\n",
    "    return recs_df.withColumn(\"rank\", F.row_number().over(window)) \\\n",
    "        .filter(F.col(\"rank\") <= k) \\\n",
    "        .groupBy(\"user_id\").agg(F.collect_list(\"item_id\").alias(\"recommended_items\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "eu_sXcNwh518"
   },
   "outputs": [],
   "source": [
    "def precision_at_k(top_k_df, ground_truth_df, k):\n",
    "    joined = top_k_df.join(ground_truth_df, \"user_id\")\n",
    "    result = joined.withColumn(\"hits\", F.size(F.array_intersect(\"recommended_items\", \"relevant_items\"))) \\\n",
    "        .agg(F.avg(F.col(\"hits\") / k)).collect()[0][0]\n",
    "    return result or 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "bMbB0jRrh519"
   },
   "outputs": [],
   "source": [
    "def recall_at_k(top_k_df, ground_truth_df):\n",
    "    joined = top_k_df.join(ground_truth_df, \"user_id\")\n",
    "    result = joined.withColumn(\"hits\", F.size(F.array_intersect(\"recommended_items\", \"relevant_items\"))) \\\n",
    "        .withColumn(\"recall\", F.when(F.size(\"relevant_items\") > 0, F.col(\"hits\") / F.size(\"relevant_items\")).otherwise(0)) \\\n",
    "        .agg(F.avg(\"recall\")).collect()[0][0]\n",
    "    return result or 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "LqilPysEh519"
   },
   "outputs": [],
   "source": [
    "def ndcg_at_k(top_k_df, ground_truth_df, k):\n",
    "    joined = top_k_df.join(ground_truth_df, \"user_id\")\n",
    "    exploded = joined.select(\n",
    "        \"user_id\",\n",
    "        \"relevant_items\",\n",
    "        F.posexplode(\"recommended_items\").alias(\"pos\", \"item_id\")\n",
    "    ).withColumn(\"item_id\", F.col(\"item_id\").cast(IntegerType()))\n",
    "\n",
    "    with_dcg = exploded \\\n",
    "        .withColumn(\"rel\", F.when(F.array_contains(\"relevant_items\", F.col(\"item_id\")), 1.0).otherwise(0.0)) \\\n",
    "        .withColumn(\"dcg\", F.col(\"rel\") / F.log2(F.col(\"pos\") + 2)) \\\n",
    "        .groupBy(\"user_id\", \"relevant_items\").agg(F.sum(\"dcg\").alias(\"dcg\"))\n",
    "\n",
    "    idcg_vals = [sum(1.0 / log2(i + 2) for i in range(n)) for n in range(k + 1)]\n",
    "    idcg_map = F.create_map(*[x for i, v in enumerate(idcg_vals) for x in (F.lit(i), F.lit(v))])\n",
    "\n",
    "    result = with_dcg \\\n",
    "        .withColumn(\"num_rel\", F.least(F.size(\"relevant_items\"), F.lit(k))) \\\n",
    "        .withColumn(\"idcg\", idcg_map[F.col(\"num_rel\")]) \\\n",
    "        .withColumn(\"ndcg\", F.when(F.col(\"idcg\") > 0, F.col(\"dcg\") / F.col(\"idcg\")).otherwise(0)) \\\n",
    "        .agg(F.avg(\"ndcg\")).collect()[0][0]\n",
    "    return result or 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Fb28KwU6h51_"
   },
   "outputs": [],
   "source": [
    "def evaluate(recs_df, score_col, name):\n",
    "    if recs_df.count() == 0:\n",
    "        print(f\"{name}: No recommendations (not implemented)\")\n",
    "        return {\"Precision@10\": 0.0, \"Recall@10\": 0.0, \"NDCG@10\": 0.0}\n",
    "\n",
    "    top_k = get_top_k(recs_df, score_col, K)\n",
    "    p = precision_at_k(top_k, ground_truth, K)\n",
    "    r = recall_at_k(top_k, ground_truth)\n",
    "    n = ndcg_at_k(top_k, ground_truth, K)\n",
    "\n",
    "    print(f\"{name}: P@{K}={p:.4f}, R@{K}={r:.4f}, NDCG@{K}={n:.4f}\")\n",
    "    return {\"Precision@10\": p, \"Recall@10\": r, \"NDCG@10\": n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcQId4lLh52C"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4_55bcPh52C",
    "outputId": "6b5486e9-a788-4a38-b856-37791abbc479"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1016:==========================================>             (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS: P@10=0.0265, R@10=0.0194, NDCG@10=0.0249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "als_metrics = evaluate(als_recs, \"als_score\", \"ALS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "La8eg0aFh52D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1130:===============================================>    (183 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content-Based: P@10=0.0280, R@10=0.0217, NDCG@10=0.0319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "content_metrics = evaluate(content_recs, \"content_score\", \"Content-Based\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "9hpFNv0Vh52D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1687:==============>                                         (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid: P@10=0.0352, R@10=0.0240, NDCG@10=0.0359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hybrid_metrics = evaluate(hybrid_recs, \"final_score\", \"Hybrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kL3phVEuh52D"
   },
   "source": [
    "### Bonus: GBT Re-Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "eZfDCdK1h52D"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqAeFwu6h52D"
   },
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "FlHvgvrWh52D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1757:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------------------+--------------------+\n",
      "|        Model|       Precision@10|           Recall@10|             NDCG@10|\n",
      "+-------------+-------------------+--------------------+--------------------+\n",
      "|          ALS|0.02651287194918031|0.019449604815379978|0.024926569370517664|\n",
      "|Content-Based|0.02800066867268473|0.021737730303375884| 0.03185693701620872|\n",
      "|       Hybrid|0.03523905048478774|0.024030692726811353| 0.03587917595862495|\n",
      "+-------------+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summary = [\n",
    "    (\"ALS\", als_metrics[\"Precision@10\"], als_metrics[\"Recall@10\"], als_metrics[\"NDCG@10\"]),\n",
    "    (\"Content-Based\", content_metrics[\"Precision@10\"], content_metrics[\"Recall@10\"], content_metrics[\"NDCG@10\"]),\n",
    "    (\"Hybrid\", hybrid_metrics[\"Precision@10\"], hybrid_metrics[\"Recall@10\"], hybrid_metrics[\"NDCG@10\"]),\n",
    "]\n",
    "spark.createDataFrame(summary, [\"Model\", \"Precision@10\", \"Recall@10\", \"NDCG@10\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "history_visible": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mining-massive-databases",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
