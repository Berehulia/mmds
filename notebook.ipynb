{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jV-qaRjjh51v"
   },
   "source": [
    "# Hybrid Recommendation System\n",
    "\n",
    "**Team Structure:**\n",
    "- Member 1: Infrastructure, Data Loading, Fusion & Evaluation\n",
    "- Member 2: Collaborative Filtering (ALS)\n",
    "- Member 3: Content-Based Filtering (TF-IDF + LSH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z3aeJVqkPIa"
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnzLbX57kUnJ"
   },
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GMg3a2fqh51y",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:14.312304Z",
     "start_time": "2026-01-18T18:44:13.561599Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from math import log2\n",
    "\n",
    "# Fix for Windows\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, FloatType, StructType, StructField"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d--B_svch51z"
   },
   "source": [
    "### 1.2 Download Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7xDJxcgHh511",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:16.535811Z",
     "start_time": "2026-01-18T18:44:15.409363Z"
    }
   },
   "source": [
    "DATA_URL = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
    "DATA_DIR = \"data\"\n",
    "DATASET_DIR = os.path.join(DATA_DIR, \"ml-1m\")\n",
    "ZIP_PATH = os.path.join(DATA_DIR, \"ml-1m.zip\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2xaxdp_xh512",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:19.109624Z",
     "start_time": "2026-01-18T18:44:16.951545Z"
    }
   },
   "source": [
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t3cKFv2Bh513",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:22.237394Z",
     "start_time": "2026-01-18T18:44:19.816138Z"
    }
   },
   "source": [
    "if not os.path.exists(DATASET_DIR):\n",
    "\n",
    "    if not os.path.exists(ZIP_PATH):\n",
    "        print(\"Downloading MovieLens ml-1m...\")\n",
    "        urllib.request.urlretrieve(DATA_URL, ZIP_PATH)\n",
    "\n",
    "    print(\"Extracting...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew2EJ0KCh513"
   },
   "source": [
    "### 1.3 Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zH6tSX_bh513",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:31.728242Z",
     "start_time": "2026-01-18T18:44:22.276387Z"
    }
   },
   "source": [
    "spark = SparkSession.builder.appName(\"MMDS\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"20\") \\\n",
    "    .config(\"spark.default.parallelism\", \"20\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"400m\") \\\n",
    "    .getOrCreate()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/18 20:44:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y95TFnL2h514"
   },
   "source": [
    "### 1.4 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBLTLe1th514",
    "outputId": "fc2b2efd-dfe3-4498-e34a-ba1de16a3140",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:34.114384Z",
     "start_time": "2026-01-18T18:44:31.741706Z"
    }
   },
   "source": [
    "users_df = spark.read.text(os.path.join(DATASET_DIR, \"users.dat\")).select(\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(0).cast(IntegerType()).alias(\"user_id\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(1).alias(\"gender\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(2).cast(IntegerType()).alias(\"age\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(3).cast(IntegerType()).alias(\"occupation\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(4).alias(\"zip_code\")\n",
    ")\n",
    "users_df.count()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6040"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFLr1EX2h514",
    "outputId": "08e9e755-c1b5-4160-d625-643f128fc28e",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:34.391156Z",
     "start_time": "2026-01-18T18:44:34.144296Z"
    }
   },
   "source": [
    "items_df = spark.read.text(os.path.join(DATASET_DIR, \"movies.dat\")).select(\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(0).cast(IntegerType()).alias(\"item_id\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(1).alias(\"title\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(2).alias(\"genres\")\n",
    ")\n",
    "items_df.count()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uafyQhs3h515",
    "outputId": "1af79a1e-18be-442c-d72f-fa38cf29fbed",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:34.636657Z",
     "start_time": "2026-01-18T18:44:34.402272Z"
    }
   },
   "source": [
    "ratings_df = spark.read.text(os.path.join(DATASET_DIR, \"ratings.dat\")).select(\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(0).cast(IntegerType()).alias(\"user_id\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(1).cast(IntegerType()).alias(\"item_id\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(2).cast(FloatType()).alias(\"rating\"),\n",
    "    F.split(F.col(\"value\"), \"::\").getItem(3).cast(IntegerType()).alias(\"timestamp\")\n",
    ")\n",
    "ratings_df.count()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000209"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36xJVL9Ih515",
    "outputId": "30aaf041-8590-4453-c239-224def2157c4",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:34.892893Z",
     "start_time": "2026-01-18T18:44:34.662999Z"
    }
   },
   "source": [
    "users_df.show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+----------+--------+\n",
      "|user_id|gender|age|occupation|zip_code|\n",
      "+-------+------+---+----------+--------+\n",
      "|      1|     F|  1|        10|   48067|\n",
      "|      2|     M| 56|        16|   70072|\n",
      "|      3|     M| 25|        15|   55117|\n",
      "|      4|     M| 45|         7|   02460|\n",
      "|      5|     M| 25|        20|   55455|\n",
      "+-------+------+---+----------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtXD-ucvh515",
    "outputId": "fe9aaea5-5745-46bc-aa52-f8438ba62fe5",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:35.157828Z",
     "start_time": "2026-01-18T18:44:34.934896Z"
    }
   },
   "source": [
    "items_df.show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|item_id|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Animation|Childre...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|        Comedy|Drama|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BI5_IghMh515",
    "outputId": "e390641c-b757-445d-af40-8b20d25ed3a9",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:35.399507Z",
     "start_time": "2026-01-18T18:44:35.196968Z"
    }
   },
   "source": [
    "ratings_df.show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+---------+\n",
      "|user_id|item_id|rating|timestamp|\n",
      "+-------+-------+------+---------+\n",
      "|      1|   1193|   5.0|978300760|\n",
      "|      1|    661|   3.0|978302109|\n",
      "|      1|    914|   3.0|978301968|\n",
      "|      1|   3408|   4.0|978300275|\n",
      "|      1|   2355|   5.0|978824291|\n",
      "+-------+-------+------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYsOYCQEh515"
   },
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmGEbwy3mDDY"
   },
   "source": [
    "### 2.1 Rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cARZQAvGh515",
    "outputId": "aa9e00a1-9cec-4884-c3e8-bce725927b73",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:35.599742Z",
     "start_time": "2026-01-18T18:44:35.413737Z"
    }
   },
   "source": [
    "num_users = users_df.count()\n",
    "num_items = items_df.count()\n",
    "num_ratings = ratings_df.count()\n",
    "sparsity = (1 - (num_ratings / (num_users * num_items))) * 100\n",
    "\n",
    "print(f\"Users:            {num_users:,}\")\n",
    "print(f\"Movies:           {num_items:,}\")\n",
    "print(f\"Ratings:          {num_ratings:,}\")\n",
    "print(f\"Sparsity:         {sparsity:.2f}%\")\n",
    "print(f\"Avg ratings/user: {num_ratings/num_users:.1f}\")\n",
    "print(f\"Avg ratings/movie:{num_ratings/num_items:.1f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users:            6,040\n",
      "Movies:           3,883\n",
      "Ratings:          1,000,209\n",
      "Sparsity:         95.74%\n",
      "Avg ratings/user: 165.6\n",
      "Avg ratings/movie:257.6\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQaH_GJVmLpj"
   },
   "source": [
    "### 2.2 Rating distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlDMVOJNh515",
    "outputId": "dd79f2eb-46dd-4966-cbbc-0daebd89857e",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:36.697505Z",
     "start_time": "2026-01-18T18:44:35.602681Z"
    }
   },
   "source": [
    "ratings_df.groupBy(\"rating\").count().orderBy(\"rating\").show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|rating| count|\n",
      "+------+------+\n",
      "|   1.0| 56174|\n",
      "|   2.0|107557|\n",
      "|   3.0|261197|\n",
      "|   4.0|348971|\n",
      "|   5.0|226310|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMl6TOXvmQiN"
   },
   "source": [
    "### 2.3 Genre distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4uolq8hkh515",
    "outputId": "8a63ec2b-7dc1-4c5a-9a12-390dbc0b070c",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:37.496027Z",
     "start_time": "2026-01-18T18:44:37.266359Z"
    }
   },
   "source": [
    "items_df.select(F.explode(F.split(F.col(\"genres\"), \"\\\\|\")).alias(\"genre\")) \\\n",
    "    .groupBy(\"genre\").count().orderBy(F.desc(\"count\")).show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|      genre|count|\n",
      "+-----------+-----+\n",
      "|      Drama| 1603|\n",
      "|     Comedy| 1200|\n",
      "|     Action|  503|\n",
      "|   Thriller|  492|\n",
      "|    Romance|  471|\n",
      "|     Horror|  343|\n",
      "|  Adventure|  283|\n",
      "|     Sci-Fi|  276|\n",
      "| Children's|  251|\n",
      "|      Crime|  211|\n",
      "|        War|  143|\n",
      "|Documentary|  127|\n",
      "|    Musical|  114|\n",
      "|    Mystery|  106|\n",
      "|  Animation|  105|\n",
      "|    Western|   68|\n",
      "|    Fantasy|   68|\n",
      "|  Film-Noir|   44|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M4BhkBOmoPr"
   },
   "source": [
    "### 2.4 User gender distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWtkOG1rh515",
    "outputId": "1955049f-0ef8-40c7-8568-765dfc3d126c",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:37.737061Z",
     "start_time": "2026-01-18T18:44:37.531898Z"
    }
   },
   "source": [
    "users_df.groupBy(\"gender\").count().show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     F| 1709|\n",
      "|     M| 4331|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_t-YXlxmtrw"
   },
   "source": [
    "### 2.5 User age distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tt5JfAqKh515",
    "outputId": "5e18a191-b997-4b90-fcb5-9576ecaafb17",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:37.975603Z",
     "start_time": "2026-01-18T18:44:37.752743Z"
    }
   },
   "source": [
    "users_df.groupBy(\"age\").count().orderBy(\"age\").show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "|  1|  222|\n",
      "| 18| 1103|\n",
      "| 25| 2096|\n",
      "| 35| 1193|\n",
      "| 45|  550|\n",
      "| 50|  496|\n",
      "| 56|  380|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWmkYV91h516"
   },
   "source": [
    "## 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r2lL2tL-h516",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:38.017969Z",
     "start_time": "2026-01-18T18:44:38.006382Z"
    }
   },
   "source": [
    "def chronological_split(ratings_df, train_ratio=0.8, min_train_ratings=5):\n",
    "    user_time_window = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "    user_count_window = Window.partitionBy(\"user_id\")\n",
    "\n",
    "    ratings_with_rank = ratings_df.withColumn(\n",
    "        \"row_num\", F.row_number().over(user_time_window)\n",
    "    ).withColumn(\n",
    "        \"user_total\", F.count(\"*\").over(user_count_window)\n",
    "    ).withColumn(\n",
    "        \"train_threshold\", F.floor(F.col(\"user_total\") * train_ratio)\n",
    "    )\n",
    "\n",
    "    ratings_valid = ratings_with_rank.filter(\n",
    "        F.col(\"train_threshold\") >= min_train_ratings\n",
    "    )\n",
    "\n",
    "    ratings_labeled = ratings_valid.withColumn(\n",
    "        \"split\",\n",
    "        F.when(F.col(\"row_num\") <= F.col(\"train_threshold\"), \"train\").otherwise(\"test\")\n",
    "    )\n",
    "\n",
    "    original_columns = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
    "    train_df = ratings_labeled.filter(F.col(\"split\") == \"train\").select(original_columns)\n",
    "    test_df = ratings_labeled.filter(F.col(\"split\") == \"test\").select(original_columns)\n",
    "\n",
    "    return train_df, test_df"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7bz6lAKh516",
    "outputId": "43cfc1af-6a46-4cdf-f794-ca94e38ad55a",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:38.130979Z",
     "start_time": "2026-01-18T18:44:38.020771Z"
    }
   },
   "source": [
    "train_df, test_df = chronological_split(ratings_df, train_ratio=0.8, min_train_ratings=5)\n",
    "train_df = train_df.cache()\n",
    "test_df = test_df.cache()"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ambqvc_Fh516",
    "outputId": "a85471a3-77c0-4640-d912-043c97d3bca4",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:40.608304Z",
     "start_time": "2026-01-18T18:44:38.132235Z"
    }
   },
   "source": [
    "print(f\"Train: {train_df.count():,} ratings\")\n",
    "print(f\"Test: {test_df.count():,} ratings\")\n",
    "print(f\"Users in train: {train_df.select('user_id').distinct().count():,}\")\n",
    "print(f\"Users in test: {test_df.select('user_id').distinct().count():,}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 797,758 ratings\n",
      "Test: 202,451 ratings\n",
      "Users in train: 6,040\n",
      "Users in test: 6,040\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:40.692839Z",
     "start_time": "2026-01-18T18:44:40.646828Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Evaluation Code\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.1 Hyperparameters"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:41.120776Z",
     "start_time": "2026-01-18T18:44:40.701892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "K = 10\n",
    "RELEVANCE_THRESHOLD = 4.0"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 Ground Truth"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:41.685458Z",
     "start_time": "2026-01-18T18:44:41.529151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ground_truth = test_df.filter(F.col(\"rating\") >= RELEVANCE_THRESHOLD) \\\n",
    "    .groupBy(\"user_id\").agg(F.collect_list(\"item_id\").alias(\"relevant_items\"))"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.3 Functions"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:41.756243Z",
     "start_time": "2026-01-18T18:44:41.737704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_top_k(recs_df, score_col, k):\n",
    "    window = Window.partitionBy(\"user_id\").orderBy(F.desc(score_col))\n",
    "    ranked = recs_df.withColumn(\"rank\", F.row_number().over(window)) \\\n",
    "        .filter(F.col(\"rank\") <= k)\n",
    "\n",
    "    return ranked.groupBy(\"user_id\").agg(\n",
    "        F.array_sort(F.collect_list(F.struct(\"rank\", \"item_id\"))).alias(\"ranked_structs\")\n",
    "    ).withColumn(\n",
    "        \"recommended_items\",\n",
    "        F.expr(\"transform(ranked_structs, x -> x.item_id)\")\n",
    "    ).drop(\"ranked_structs\")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:41.764224Z",
     "start_time": "2026-01-18T18:44:41.757817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def precision_at_k(top_k_df, ground_truth_df, k):\n",
    "    joined = top_k_df.join(ground_truth_df, \"user_id\")\n",
    "    result = joined.withColumn(\"hits\", F.size(F.array_intersect(\"recommended_items\", \"relevant_items\"))) \\\n",
    "        .agg(F.avg(F.col(\"hits\") / k)).collect()[0][0]\n",
    "    return result or 0.0"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:41.771342Z",
     "start_time": "2026-01-18T18:44:41.764675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def recall_at_k(top_k_df, ground_truth_df):\n",
    "    joined = top_k_df.join(ground_truth_df, \"user_id\")\n",
    "    result = joined.withColumn(\"hits\", F.size(F.array_intersect(\"recommended_items\", \"relevant_items\"))) \\\n",
    "        .withColumn(\"recall\", F.when(F.size(\"relevant_items\") > 0, F.col(\"hits\") / F.size(\"relevant_items\")).otherwise(0)) \\\n",
    "        .agg(F.avg(\"recall\")).collect()[0][0]\n",
    "    return result or 0.0"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:41.784535Z",
     "start_time": "2026-01-18T18:44:41.771764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ndcg_at_k(top_k_df, ground_truth_df, k):\n",
    "    joined = top_k_df.join(ground_truth_df, \"user_id\")\n",
    "    exploded = joined.select(\n",
    "        \"user_id\",\n",
    "        \"relevant_items\",\n",
    "        F.posexplode(\"recommended_items\").alias(\"pos\", \"item_id\")\n",
    "    ).withColumn(\"item_id\", F.col(\"item_id\").cast(IntegerType()))\n",
    "\n",
    "    with_dcg = exploded \\\n",
    "        .withColumn(\"rel\", F.when(F.array_contains(\"relevant_items\", F.col(\"item_id\")), 1.0).otherwise(0.0)) \\\n",
    "        .withColumn(\"dcg\", F.col(\"rel\") / F.log2(F.col(\"pos\") + 2)) \\\n",
    "        .groupBy(\"user_id\", \"relevant_items\").agg(F.sum(\"dcg\").alias(\"dcg\"))\n",
    "\n",
    "    idcg_vals = [sum(1.0 / log2(i + 2) for i in range(n)) for n in range(k + 1)]\n",
    "    idcg_map = F.create_map(*[x for i, v in enumerate(idcg_vals) for x in (F.lit(i), F.lit(v))])\n",
    "\n",
    "    result = with_dcg \\\n",
    "        .withColumn(\"num_rel\", F.least(F.size(\"relevant_items\"), F.lit(k))) \\\n",
    "        .withColumn(\"idcg\", idcg_map[F.col(\"num_rel\")]) \\\n",
    "        .withColumn(\"ndcg\", F.when(F.col(\"idcg\") > 0, F.col(\"dcg\") / F.col(\"idcg\")).otherwise(0)) \\\n",
    "        .agg(F.avg(\"ndcg\")).collect()[0][0]\n",
    "    return result or 0.0"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:41.791819Z",
     "start_time": "2026-01-18T18:44:41.785891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(recs_df, score_col, name):\n",
    "    if recs_df.count() == 0:\n",
    "        print(f\"{name}: No recommendations (not implemented)\")\n",
    "        return {\"Precision@10\": 0.0, \"Recall@10\": 0.0, \"NDCG@10\": 0.0}\n",
    "\n",
    "    top_k = get_top_k(recs_df, score_col, K)\n",
    "    p = precision_at_k(top_k, ground_truth, K)\n",
    "    r = recall_at_k(top_k, ground_truth)\n",
    "    n = ndcg_at_k(top_k, ground_truth, K)\n",
    "\n",
    "    print(f\"{name}: P@{K}={p:.4f}, R@{K}={r:.4f}, NDCG@{K}={n:.4f}\")\n",
    "    return {\"Precision@10\": p, \"Recall@10\": r, \"NDCG@10\": n}"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZBXPcymh517"
   },
   "source": [
    "## 5. Collaborative Filtering (ALS)\n",
    "\n",
    "Implement using `pyspark.ml.recommendation.ALS`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tR9qfStQh517",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:41.799707Z",
     "start_time": "2026-01-18T18:44:41.792592Z"
    }
   },
   "source": [
    "class CollaborativeFilter:\n",
    "\n",
    "    def __init__(self, rank=10, regParam=0.1, maxIter=10):\n",
    "        self.als = ALS(\n",
    "            userCol=\"user_id\",\n",
    "            itemCol=\"item_id\",\n",
    "            ratingCol=\"rating\",\n",
    "            rank=rank,\n",
    "            regParam=regParam,\n",
    "            maxIter=maxIter,\n",
    "            coldStartStrategy=\"drop\",\n",
    "            nonnegative=True\n",
    "        )\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, df):\n",
    "        self.model = self.als.fit(df)\n",
    "\n",
    "    def get_recommendations(self, df, k=10):\n",
    "        \"\"\"Get top-K recommendations\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Call train() first.\")\n",
    "\n",
    "        users = df.select(\"user_id\").distinct()\n",
    "        user_recs = self.model.recommendForUserSubset(users, k)\n",
    "\n",
    "        return user_recs.select(\n",
    "            F.col(\"user_id\"),\n",
    "            F.explode(\"recommendations\").alias(\"rec\")\n",
    "        ).select(\n",
    "            F.col(\"user_id\"),\n",
    "            F.col(\"rec.item_id\").cast(IntegerType()).alias(\"item_id\"),\n",
    "            F.col(\"rec.rating\").alias(\"prediction\")\n",
    "        )\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"Predict ratings for user-item pairs in test\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Train should be called first\")\n",
    "        return self.model.transform(df)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxfposUDh517"
   },
   "source": [
    "### Bonus: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bmVnX8K7h517",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:42.536953Z",
     "start_time": "2026-01-18T18:44:41.800253Z"
    }
   },
   "source": [
    "#\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "# --- Inner split for tuning (chronological, like your main split)\n",
    "train_inner, val_df = chronological_split(train_df, train_ratio=0.9, min_train_ratings=5)\n",
    "\n",
    "# Ground truth = relevant in our validation set\n",
    "val_ground_truth = val_df.filter(F.col(\"rating\") >= RELEVANCE_THRESHOLD) \\\n",
    "    .groupBy(\"user_id\").agg(F.collect_list(\"item_id\").alias(\"relevant_items\"))\n",
    "\n",
    "# Cache\n",
    "train_inner.cache()\n",
    "val_df.cache()\n",
    "_ = train_inner.count()\n",
    "_ = val_df.count()\n",
    "\n",
    "def eval_als_on_val(rank, regParam, maxIter, k_candidates=100):\n",
    "    \"\"\"\n",
    "    Train ALS on train_inner, recommend on val_df, compute ranking metrics\n",
    "    vs val_ground_truth.\n",
    "    \"\"\"\n",
    "    cf = CollaborativeFilter(rank=rank, regParam=regParam, maxIter=maxIter)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    cf.train(train_inner)\n",
    "    train_s = time.perf_counter() - t0\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    recs = cf.get_recommendations(val_df, k=k_candidates).withColumnRenamed(\"prediction\", \"als_score\").cache()\n",
    "    _ = recs.count()  # materialize\n",
    "    infer_s = time.perf_counter() - t0\n",
    "\n",
    "    # metrics@K\n",
    "    top_k = get_top_k(recs, \"als_score\", K)\n",
    "    p = precision_at_k(top_k, val_ground_truth, K)\n",
    "    r = recall_at_k(top_k, val_ground_truth)\n",
    "    n = ndcg_at_k(top_k, val_ground_truth, K)\n",
    "\n",
    "    return {\n",
    "        \"rank\": rank, \"regParam\": regParam, \"maxIter\": maxIter,\n",
    "        \"P@K\": p, \"R@K\": r, \"NDCG@K\": n,\n",
    "        \"train_s\": train_s, \"infer_s\": infer_s,\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:44:42.638706Z",
     "start_time": "2026-01-18T18:44:42.565197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_best_als_params():\n",
    "    ranks = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    regParams = [0.01, 0.05, 0.1, 0.2]\n",
    "    maxIters = [10, 20, 30]\n",
    "\n",
    "    als_tune_results = []\n",
    "    als_best = None\n",
    "\n",
    "    for rank, reg, iters in product(ranks, regParams, maxIters):\n",
    "        out = eval_als_on_val(rank, reg, iters, k_candidates=100)\n",
    "        als_tune_results.append(out)\n",
    "\n",
    "        if als_best is None or out[\"NDCG@K\"] > als_best[\"NDCG@K\"]:\n",
    "            als_best = out\n",
    "\n",
    "        print(\n",
    "            f\"ALS tune rank={rank:>2}, reg={reg:<4}, iters={iters:<2} | \"\n",
    "            f\"NDCG@{K}={out['NDCG@K']:.4f} P@{K}={out['P@K']:.4f} R@{K}={out['R@K']:.4f} | \"\n",
    "            f\"train={out['train_s']:.2f}s infer={out['infer_s']:.2f}s\"\n",
    "        )\n",
    "\n",
    "# re-running it is lenghty. Uncomment it on your own risk\n",
    "# als_best = find_best_als_params()\n",
    "als_best = {\"rank\": 80, \"regParam\": 0.05, \"maxIter\": 30}\n",
    "print(\"\\nBEST ALS PARAMS (by NDCG@K):\")\n",
    "print(als_best)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEST ALS PARAMS (by NDCG@K):\n",
      "{'rank': 80, 'regParam': 0.05, 'maxIter': 30}\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<pre>\n",
    "\n",
    "ALS tune rank=10, reg=0.01, iters=10 | NDCG@10=0.0005 P@10=0.0004 R@10=0.0006 | train=3.37s infer=4.49s\n",
    "\n",
    "\n",
    "ALS tune rank=10, reg=0.01, iters=20 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0009 | train=2.53s infer=2.65s\n",
    "\n",
    "\n",
    "ALS tune rank=10, reg=0.01, iters=30 | NDCG@10=0.0009 P@10=0.0010 R@10=0.0012 | train=3.96s infer=3.06s\n",
    "\n",
    "\n",
    "ALS tune rank=10, reg=0.05, iters=10 | NDCG@10=0.0030 P@10=0.0028 R@10=0.0047 | train=2.17s infer=3.19s\n",
    "\n",
    "\n",
    "ALS tune rank=10, reg=0.05, iters=20 | NDCG@10=0.0052 P@10=0.0047 R@10=0.0072 | train=2.67s infer=3.27s\n",
    "\n",
    "\n",
    "ALS tune rank=10, reg=0.05, iters=30 | NDCG@10=0.0064 P@10=0.0057 R@10=0.0086 | train=3.36s infer=1.85s\n",
    "\n",
    "\n",
    "ALS tune rank=10, reg=0.1 , iters=10 | NDCG@10=0.0037 P@10=0.0035 R@10=0.0062 | train=2.52s infer=2.71s\n",
    "\n",
    "\n",
    "ALS tune rank=10, reg=0.1 , iters=20 | NDCG@10=0.0069 P@10=0.0063 R@10=0.0105 | train=2.58s infer=4.16s\n",
    "\n",
    "\n",
    "ALS tune rank=10, reg=0.1 , iters=30 | NDCG@10=0.0078 P@10=0.0071 R@10=0.0116 | train=4.65s infer=3.38s\n",
    "\n",
    "\n",
    "ALS tune rank=10, reg=0.2 , iters=10 | NDCG@10=0.0003 P@10=0.0003 R@10=0.0004 | train=1.57s infer=2.45s\n",
    "\n",
    "\n",
    "ALS tune rank=10, reg=0.2 , iters=20 | NDCG@10=0.0006 P@10=0.0006 R@10=0.0011 | train=2.58s infer=2.73s\n",
    "\n",
    "\n",
    "ALS tune rank=10, reg=0.2 , iters=30 | NDCG@10=0.0006 P@10=0.0007 R@10=0.0012 | train=4.23s infer=3.12s\n",
    "\n",
    "\n",
    "ALS tune rank=20, reg=0.01, iters=10 | NDCG@10=0.0016 P@10=0.0017 R@10=0.0013 | train=1.97s infer=2.56s\n",
    "\n",
    "\n",
    "ALS tune rank=20, reg=0.01, iters=20 | NDCG@10=0.0023 P@10=0.0023 R@10=0.0023 | train=4.28s infer=3.33s\n",
    "\n",
    "\n",
    "ALS tune rank=20, reg=0.01, iters=30 | NDCG@10=0.0026 P@10=0.0027 R@10=0.0026 | train=5.39s infer=3.46s\n",
    "\n",
    "\n",
    "ALS tune rank=20, reg=0.05, iters=10 | NDCG@10=0.0087 P@10=0.0069 R@10=0.0113 | train=2.35s infer=2.83s\n",
    "\n",
    "\n",
    "ALS tune rank=20, reg=0.05, iters=20 | NDCG@10=0.0104 P@10=0.0083 R@10=0.0129 | train=3.65s infer=3.38s\n",
    "\n",
    "\n",
    "ALS tune rank=20, reg=0.05, iters=30 | NDCG@10=0.0110 P@10=0.0086 R@10=0.0135 | train=4.86s infer=3.17s\n",
    "\n",
    "\n",
    "ALS tune rank=20, reg=0.1 , iters=10 | NDCG@10=0.0068 P@10=0.0062 R@10=0.0105 | train=2.31s infer=2.67s\n",
    "\n",
    "\n",
    "ALS tune rank=20, reg=0.1 , iters=20 | NDCG@10=0.0100 P@10=0.0087 R@10=0.0142 | train=2.86s infer=3.25s\n",
    "\n",
    "\n",
    "ALS tune rank=20, reg=0.1 , iters=30 | NDCG@10=0.0112 P@10=0.0094 R@10=0.0151 | train=4.99s infer=3.13s\n",
    "\n",
    "\n",
    "ALS tune rank=20, reg=0.2 , iters=10 | NDCG@10=0.0005 P@10=0.0005 R@10=0.0009 | train=2.32s infer=3.49s\n",
    "\n",
    "\n",
    "ALS tune rank=20, reg=0.2 , iters=20 | NDCG@10=0.0006 P@10=0.0007 R@10=0.0012 | train=3.59s infer=3.01s\n",
    "\n",
    "\n",
    "ALS tune rank=20, reg=0.2 , iters=30 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=4.73s infer=3.05s\n",
    "\n",
    "\n",
    "ALS tune rank=30, reg=0.01, iters=10 | NDCG@10=0.0033 P@10=0.0031 R@10=0.0027 | train=2.68s infer=2.63s\n",
    "\n",
    "\n",
    "ALS tune rank=30, reg=0.01, iters=20 | NDCG@10=0.0042 P@10=0.0037 R@10=0.0041 | train=4.47s infer=2.66s\n",
    "\n",
    "\n",
    "ALS tune rank=30, reg=0.01, iters=30 | NDCG@10=0.0044 P@10=0.0040 R@10=0.0041 | train=6.52s infer=2.93s\n",
    "\n",
    "\n",
    "ALS tune rank=30, reg=0.05, iters=10 | NDCG@10=0.0118 P@10=0.0093 R@10=0.0145 | train=2.87s infer=2.09s\n",
    "\n",
    "\n",
    "ALS tune rank=30, reg=0.05, iters=20 | NDCG@10=0.0142 P@10=0.0106 R@10=0.0168 | train=3.98s infer=2.56s\n",
    "\n",
    "\n",
    "ALS tune rank=30, reg=0.05, iters=30 | NDCG@10=0.0143 P@10=0.0109 R@10=0.0165 | train=6.52s infer=2.99s\n",
    "\n",
    "\n",
    "ALS tune rank=30, reg=0.1 , iters=10 | NDCG@10=0.0079 P@10=0.0069 R@10=0.0121 | train=2.67s infer=3.21s\n",
    "\n",
    "\n",
    "ALS tune rank=30, reg=0.1 , iters=20 | NDCG@10=0.0111 P@10=0.0094 R@10=0.0155 | train=5.59s infer=3.68s\n",
    "\n",
    "\n",
    "ALS tune rank=30, reg=0.1 , iters=30 | NDCG@10=0.0122 P@10=0.0103 R@10=0.0165 | train=6.32s infer=2.25s\n",
    "\n",
    "\n",
    "ALS tune rank=30, reg=0.2 , iters=10 | NDCG@10=0.0004 P@10=0.0004 R@10=0.0006 | train=1.72s infer=1.92s\n",
    "\n",
    "\n",
    "ALS tune rank=30, reg=0.2 , iters=20 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=3.25s infer=2.31s\n",
    "\n",
    "\n",
    "ALS tune rank=30, reg=0.2 , iters=30 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=5.43s infer=2.19s\n",
    "\n",
    "\n",
    "ALS tune rank=40, reg=0.01, iters=10 | NDCG@10=0.0038 P@10=0.0037 R@10=0.0036 | train=2.41s infer=1.92s\n",
    "\n",
    "\n",
    "ALS tune rank=40, reg=0.01, iters=20 | NDCG@10=0.0060 P@10=0.0054 R@10=0.0054 | train=5.72s infer=2.94s\n",
    "\n",
    "\n",
    "ALS tune rank=40, reg=0.01, iters=30 | NDCG@10=0.0060 P@10=0.0053 R@10=0.0058 | train=8.32s infer=3.01s\n",
    "\n",
    "\n",
    "ALS tune rank=40, reg=0.05, iters=10 | NDCG@10=0.0143 P@10=0.0109 R@10=0.0164 | train=3.48s infer=3.22s\n",
    "\n",
    "\n",
    "ALS tune rank=40, reg=0.05, iters=20 | NDCG@10=0.0153 P@10=0.0115 R@10=0.0176 | train=4.94s infer=2.68s\n",
    "\n",
    "\n",
    "ALS tune rank=40, reg=0.05, iters=30 | NDCG@10=0.0158 P@10=0.0118 R@10=0.0181 | train=7.69s infer=2.96s\n",
    "\n",
    "\n",
    "ALS tune rank=40, reg=0.1 , iters=10 | NDCG@10=0.0087 P@10=0.0073 R@10=0.0130 | train=2.94s infer=3.14s\n",
    "\n",
    "\n",
    "ALS tune rank=40, reg=0.1 , iters=20 | NDCG@10=0.0120 P@10=0.0099 R@10=0.0156 | train=6.40s infer=3.53s\n",
    "\n",
    "\n",
    "ALS tune rank=40, reg=0.1 , iters=30 | NDCG@10=0.0128 P@10=0.0104 R@10=0.0165 | train=7.70s infer=3.80s\n",
    "\n",
    "\n",
    "ALS tune rank=40, reg=0.2 , iters=10 | NDCG@10=0.0005 P@10=0.0004 R@10=0.0009 | train=3.05s infer=3.30s\n",
    "\n",
    "\n",
    "ALS tune rank=40, reg=0.2 , iters=20 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=4.60s infer=3.00s\n",
    "\n",
    "\n",
    "ALS tune rank=40, reg=0.2 , iters=30 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=7.12s infer=3.37s\n",
    "\n",
    "\n",
    "ALS tune rank=50, reg=0.01, iters=10 | NDCG@10=0.0052 P@10=0.0046 R@10=0.0046 | train=4.07s infer=3.17s\n",
    "\n",
    "\n",
    "ALS tune rank=50, reg=0.01, iters=20 | NDCG@10=0.0066 P@10=0.0059 R@10=0.0065 | train=6.87s infer=3.06s\n",
    "\n",
    "\n",
    "ALS tune rank=50, reg=0.01, iters=30 | NDCG@10=0.0075 P@10=0.0062 R@10=0.0075 | train=10.16s infer=2.83s\n",
    "\n",
    "\n",
    "ALS tune rank=50, reg=0.05, iters=10 | NDCG@10=0.0147 P@10=0.0108 R@10=0.0179 | train=3.73s infer=3.33s\n",
    "\n",
    "\n",
    "ALS tune rank=50, reg=0.05, iters=20 | NDCG@10=0.0164 P@10=0.0119 R@10=0.0190 | train=6.03s infer=3.08s\n",
    "\n",
    "\n",
    "ALS tune rank=50, reg=0.05, iters=30 | NDCG@10=0.0171 P@10=0.0122 R@10=0.0196 | train=9.21s infer=3.05s\n",
    "\n",
    "\n",
    "ALS tune rank=50, reg=0.1 , iters=10 | NDCG@10=0.0091 P@10=0.0078 R@10=0.0134 | train=3.48s infer=2.95s\n",
    "\n",
    "\n",
    "ALS tune rank=50, reg=0.1 , iters=20 | NDCG@10=0.0125 P@10=0.0102 R@10=0.0165 | train=5.85s infer=3.33s\n",
    "\n",
    "\n",
    "ALS tune rank=50, reg=0.1 , iters=30 | NDCG@10=0.0137 P@10=0.0110 R@10=0.0176 | train=8.61s infer=3.15s\n",
    "\n",
    "\n",
    "ALS tune rank=50, reg=0.2 , iters=10 | NDCG@10=0.0004 P@10=0.0004 R@10=0.0009 | train=3.22s infer=2.97s\n",
    "\n",
    "\n",
    "ALS tune rank=50, reg=0.2 , iters=20 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=5.47s infer=2.94s\n",
    "\n",
    "\n",
    "ALS tune rank=50, reg=0.2 , iters=30 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=8.05s infer=3.44s\n",
    "\n",
    "\n",
    "ALS tune rank=60, reg=0.01, iters=10 | NDCG@10=0.0054 P@10=0.0048 R@10=0.0050 | train=4.87s infer=3.02s\n",
    "\n",
    "\n",
    "ALS tune rank=60, reg=0.01, iters=20 | NDCG@10=0.0081 P@10=0.0067 R@10=0.0082 | train=8.53s infer=2.93s\n",
    "\n",
    "\n",
    "ALS tune rank=60, reg=0.01, iters=30 | NDCG@10=0.0089 P@10=0.0070 R@10=0.0093 | train=11.79s infer=3.30s\n",
    "\n",
    "\n",
    "ALS tune rank=60, reg=0.05, iters=10 | NDCG@10=0.0142 P@10=0.0105 R@10=0.0167 | train=4.28s infer=2.89s\n",
    "\n",
    "\n",
    "ALS tune rank=60, reg=0.05, iters=20 | NDCG@10=0.0153 P@10=0.0114 R@10=0.0178 | train=7.22s infer=3.27s\n",
    "\n",
    "\n",
    "ALS tune rank=60, reg=0.05, iters=30 | NDCG@10=0.0161 P@10=0.0119 R@10=0.0184 | train=11.17s infer=2.83s\n",
    "\n",
    "\n",
    "ALS tune rank=60, reg=0.1 , iters=10 | NDCG@10=0.0093 P@10=0.0080 R@10=0.0137 | train=4.05s infer=2.75s\n",
    "\n",
    "\n",
    "ALS tune rank=60, reg=0.1 , iters=20 | NDCG@10=0.0123 P@10=0.0103 R@10=0.0165 | train=6.97s infer=2.77s\n",
    "\n",
    "\n",
    "ALS tune rank=60, reg=0.1 , iters=30 | NDCG@10=0.0132 P@10=0.0110 R@10=0.0176 | train=9.83s infer=3.43s\n",
    "\n",
    "\n",
    "ALS tune rank=60, reg=0.2 , iters=10 | NDCG@10=0.0004 P@10=0.0004 R@10=0.0008 | train=3.82s infer=2.69s\n",
    "\n",
    "\n",
    "ALS tune rank=60, reg=0.2 , iters=20 | NDCG@10=0.0006 P@10=0.0007 R@10=0.0012 | train=6.64s infer=3.66s\n",
    "\n",
    "\n",
    "ALS tune rank=60, reg=0.2 , iters=30 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=9.28s infer=2.80s\n",
    "\n",
    "\n",
    "ALS tune rank=70, reg=0.01, iters=10 | NDCG@10=0.0077 P@10=0.0062 R@10=0.0074 | train=5.44s infer=2.72s\n",
    "\n",
    "\n",
    "ALS tune rank=70, reg=0.01, iters=20 | NDCG@10=0.0110 P@10=0.0085 R@10=0.0120 | train=10.04s infer=2.61s\n",
    "\n",
    "\n",
    "ALS tune rank=70, reg=0.01, iters=30 | NDCG@10=0.0116 P@10=0.0087 R@10=0.0114 | train=14.41s infer=3.16s\n",
    "\n",
    "\n",
    "ALS tune rank=70, reg=0.05, iters=10 | NDCG@10=0.0145 P@10=0.0107 R@10=0.0177 | train=4.86s infer=3.01s\n",
    "\n",
    "\n",
    "ALS tune rank=70, reg=0.05, iters=20 | NDCG@10=0.0164 P@10=0.0121 R@10=0.0194 | train=8.96s infer=2.81s\n",
    "\n",
    "\n",
    "ALS tune rank=70, reg=0.05, iters=30 | NDCG@10=0.0167 P@10=0.0126 R@10=0.0197 | train=12.37s infer=3.05s\n",
    "\n",
    "\n",
    "ALS tune rank=70, reg=0.1 , iters=10 | NDCG@10=0.0092 P@10=0.0078 R@10=0.0139 | train=4.48s infer=3.35s\n",
    "\n",
    "\n",
    "ALS tune rank=70, reg=0.1 , iters=20 | NDCG@10=0.0122 P@10=0.0100 R@10=0.0166 | train=8.17s infer=2.81s\n",
    "\n",
    "\n",
    "ALS tune rank=70, reg=0.1 , iters=30 | NDCG@10=0.0134 P@10=0.0108 R@10=0.0175 | train=11.81s infer=3.21s\n",
    "\n",
    "\n",
    "ALS tune rank=70, reg=0.2 , iters=10 | NDCG@10=0.0004 P@10=0.0004 R@10=0.0009 | train=4.55s infer=3.38s\n",
    "\n",
    "\n",
    "ALS tune rank=70, reg=0.2 , iters=20 | NDCG@10=0.0006 P@10=0.0007 R@10=0.0012 | train=7.64s infer=2.87s\n",
    "\n",
    "\n",
    "ALS tune rank=70, reg=0.2 , iters=30 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=10.78s infer=3.36s\n",
    "\n",
    "\n",
    "ALS tune rank=80, reg=0.01, iters=10 | NDCG@10=0.0083 P@10=0.0065 R@10=0.0084 | train=6.59s infer=2.85s\n",
    "\n",
    "\n",
    "ALS tune rank=80, reg=0.01, iters=20 | NDCG@10=0.0115 P@10=0.0088 R@10=0.0120 | train=12.08s infer=2.76s\n",
    "\n",
    "\n",
    "ALS tune rank=80, reg=0.01, iters=30 | NDCG@10=0.0125 P@10=0.0092 R@10=0.0127 | train=17.78s infer=3.15s\n",
    "\n",
    "\n",
    "ALS tune rank=80, reg=0.05, iters=10 | NDCG@10=0.0156 P@10=0.0114 R@10=0.0173 | train=5.73s infer=3.61s\n",
    "\n",
    "\n",
    "ALS tune rank=80, reg=0.05, iters=20 | NDCG@10=0.0174 P@10=0.0127 R@10=0.0200 | train=11.86s infer=2.78s\n",
    "\n",
    "\n",
    "ALS tune rank=80, reg=0.05, iters=30 | NDCG@10=0.0178 P@10=0.0129 R@10=0.0209 | train=15.38s infer=3.03s\n",
    "\n",
    "\n",
    "ALS tune rank=80, reg=0.1 , iters=10 | NDCG@10=0.0095 P@10=0.0081 R@10=0.0140 | train=5.24s infer=2.92s\n",
    "\n",
    "\n",
    "ALS tune rank=80, reg=0.1 , iters=20 | NDCG@10=0.0131 P@10=0.0108 R@10=0.0175 | train=9.90s infer=2.62s\n",
    "\n",
    "\n",
    "ALS tune rank=80, reg=0.1 , iters=30 | NDCG@10=0.0147 P@10=0.0117 R@10=0.0186 | train=14.16s infer=2.98s\n",
    "\n",
    "\n",
    "ALS tune rank=80, reg=0.2 , iters=10 | NDCG@10=0.0005 P@10=0.0005 R@10=0.0009 | train=5.03s infer=2.83s\n",
    "\n",
    "\n",
    "ALS tune rank=80, reg=0.2 , iters=20 | NDCG@10=0.0006 P@10=0.0007 R@10=0.0012 | train=9.10s infer=2.82s\n",
    "\n",
    "\n",
    "ALS tune rank=80, reg=0.2 , iters=30 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=12.87s infer=3.07s\n",
    "\n",
    "\n",
    "ALS tune rank=90, reg=0.01, iters=10 | NDCG@10=0.0088 P@10=0.0070 R@10=0.0083 | train=7.82s infer=2.76s\n",
    "\n",
    "\n",
    "ALS tune rank=90, reg=0.01, iters=20 | NDCG@10=0.0122 P@10=0.0097 R@10=0.0121 | train=14.84s infer=2.74s\n",
    "\n",
    "\n",
    "ALS tune rank=90, reg=0.01, iters=30 | NDCG@10=0.0130 P@10=0.0103 R@10=0.0138 | train=21.76s infer=2.82s\n",
    "\n",
    "\n",
    "ALS tune rank=90, reg=0.05, iters=10 | NDCG@10=0.0155 P@10=0.0114 R@10=0.0182 | train=7.10s infer=3.65s\n",
    "\n",
    "\n",
    "ALS tune rank=90, reg=0.05, iters=20 | NDCG@10=0.0167 P@10=0.0122 R@10=0.0192 | train=12.25s infer=3.18s\n",
    "\n",
    "\n",
    "ALS tune rank=90, reg=0.05, iters=30 | NDCG@10=0.0167 P@10=0.0123 R@10=0.0191 | train=18.04s infer=3.34s\n",
    "\n",
    "\n",
    "ALS tune rank=90, reg=0.1 , iters=10 | NDCG@10=0.0096 P@10=0.0081 R@10=0.0142 | train=6.38s infer=3.06s\n",
    "\n",
    "\n",
    "ALS tune rank=90, reg=0.1 , iters=20 | NDCG@10=0.0134 P@10=0.0108 R@10=0.0176 | train=11.58s infer=3.04s\n",
    "\n",
    "\n",
    "ALS tune rank=90, reg=0.1 , iters=30 | NDCG@10=0.0143 P@10=0.0113 R@10=0.0184 | train=18.57s infer=3.46s\n",
    "\n",
    "\n",
    "ALS tune rank=90, reg=0.2 , iters=10 | NDCG@10=0.0005 P@10=0.0005 R@10=0.0009 | train=6.03s infer=2.95s\n",
    "\n",
    "\n",
    "ALS tune rank=90, reg=0.2 , iters=20 | NDCG@10=0.0006 P@10=0.0007 R@10=0.0012 | train=11.30s infer=3.32s\n",
    "\n",
    "\n",
    "ALS tune rank=90, reg=0.2 , iters=30 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=15.87s infer=3.73s\n",
    "\n",
    "\n",
    "ALS tune rank=100, reg=0.01, iters=10 | NDCG@10=0.0103 P@10=0.0082 R@10=0.0100 | train=11.73s infer=3.33s\n",
    "\n",
    "\n",
    "ALS tune rank=100, reg=0.01, iters=20 | NDCG@10=0.0136 P@10=0.0103 R@10=0.0140 | train=19.53s infer=2.88s\n",
    "\n",
    "\n",
    "ALS tune rank=100, reg=0.01, iters=30 | NDCG@10=0.0143 P@10=0.0110 R@10=0.0141 | train=28.47s infer=3.02s\n",
    "\n",
    "\n",
    "ALS tune rank=100, reg=0.05, iters=10 | NDCG@10=0.0154 P@10=0.0115 R@10=0.0173 | train=7.77s infer=2.66s\n",
    "\n",
    "\n",
    "ALS tune rank=100, reg=0.05, iters=20 | NDCG@10=0.0168 P@10=0.0121 R@10=0.0189 | train=14.37s infer=2.89s\n",
    "\n",
    "\n",
    "ALS tune rank=100, reg=0.05, iters=30 | NDCG@10=0.0171 P@10=0.0123 R@10=0.0192 | train=21.23s infer=2.94s\n",
    "\n",
    "\n",
    "ALS tune rank=100, reg=0.1 , iters=10 | NDCG@10=0.0101 P@10=0.0084 R@10=0.0144 | train=7.34s infer=3.04s\n",
    "\n",
    "\n",
    "ALS tune rank=100, reg=0.1 , iters=20 | NDCG@10=0.0135 P@10=0.0108 R@10=0.0177 | train=14.60s infer=3.02s\n",
    "\n",
    "\n",
    "ALS tune rank=100, reg=0.1 , iters=30 | NDCG@10=0.0145 P@10=0.0115 R@10=0.0188 | train=20.25s infer=2.86s\n",
    "\n",
    "\n",
    "ALS tune rank=100, reg=0.2 , iters=10 | NDCG@10=0.0005 P@10=0.0005 R@10=0.0009 | train=6.53s infer=2.97s\n",
    "\n",
    "\n",
    "ALS tune rank=100, reg=0.2 , iters=20 | NDCG@10=0.0006 P@10=0.0007 R@10=0.0012 | train=12.13s infer=2.91s\n",
    "\n",
    "\n",
    "ALS tune rank=100, reg=0.2 , iters=30 | NDCG@10=0.0007 P@10=0.0007 R@10=0.0012 | train=17.06s infer=2.52s\n",
    "\n",
    "BEST ALS PARAMS (by NDCG@K):\n",
    "{'rank': 80, 'regParam': 0.05, 'maxIter': 30, 'P@K': 0.01288811795316568, 'R@K': 0.02094471027340997, 'NDCG@K': 0.01782000260548794, 'train_s': 15.377774499997031, 'infer_s': 3.02562895801384}\n",
    "l\n",
    "</pre>"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:07.643253Z",
     "start_time": "2026-01-18T18:44:42.639917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrain final ALS on full train_df with the best params\n",
    "cf = CollaborativeFilter(rank=als_best[\"rank\"], regParam=als_best[\"regParam\"], maxIter=als_best[\"maxIter\"])\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "cf.train(train_df)\n",
    "als_train_s = time.perf_counter() - t0\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "als_recs = cf.get_recommendations(test_df, k=100).withColumnRenamed(\"prediction\", \"als_score\").cache()\n",
    "als_num_recs = als_recs.count()\n",
    "als_infer_s = time.perf_counter() - t0\n",
    "\n",
    "print(f\"\\nFINAL ALS train time: {als_train_s:.2f}s\")\n",
    "print(f\"FINAL ALS inference time: {als_infer_s:.2f}s | rec rows: {als_num_recs:,}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/18 20:44:45 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "[Stage 344:==============================>                        (11 + 9) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL ALS train time: 20.46s\n",
      "FINAL ALS inference time: 4.39s | rec rows: 604,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:08.210394Z",
     "start_time": "2026-01-18T18:45:07.673718Z"
    }
   },
   "cell_type": "code",
   "source": "als_recs.show(10)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+\n",
      "|user_id|item_id|als_score|\n",
      "+-------+-------+---------+\n",
      "|     95|    260|4.6372614|\n",
      "|     95|   1198|4.6299286|\n",
      "|     95|   1250|4.5401225|\n",
      "|     95|    527|4.4914494|\n",
      "|     95|    953|4.4664316|\n",
      "|     95|    318| 4.438609|\n",
      "|     95|   1197| 4.424387|\n",
      "|     95|   2028| 4.410882|\n",
      "|     95|    110|4.3907533|\n",
      "|     95|   1304| 4.390076|\n",
      "+-------+-------+---------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMWQPlANh517"
   },
   "source": [
    "## 6. Content-Based Filtering (TF-IDF + LSH)\n",
    "\n",
    "Implement using `pyspark.ml.feature` (Tokenizer, HashingTF, IDF, BucketedRandomProjectionLSH)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:19.957974Z",
     "start_time": "2026-01-18T18:45:08.245535Z"
    }
   },
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, NGram, MinHashLSH\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import (\n",
    "    col, split, concat_ws, regexp_extract, udf, array_intersect, \n",
    "    size, sum as _sum, desc, row_number\n",
    ")\n",
    "\n",
    "class ContentBasedFilter:\n",
    "    def __init__(self, num_features=5000, bigram_features=3000, num_hash_tables=10):\n",
    "        self.num_features = num_features\n",
    "        self.bigram_features = bigram_features\n",
    "        self.num_hash_tables = num_hash_tables\n",
    "        self.jaccard_threshold = 0.1\n",
    "        self.genre_weight = 4\n",
    "        self.min_genre_overlap = 1\n",
    "        \n",
    "\n",
    "        self.lsh_model = None\n",
    "        self.vector_model = None\n",
    "        self.movies_binary = None\n",
    "        self.similar_pairs = None\n",
    "\n",
    "    def combine_binarize_udf(self):\n",
    "        def process_vectors(v1, v2):\n",
    "            if v1 is None: \n",
    "                return None\n",
    "            indices = [int(i) for i in v1.indices]\n",
    "            \n",
    "            if v2 is not None:\n",
    "                offset = int(v1.size)\n",
    "                indices += [int(i) + offset for i in v2.indices]\n",
    "                total_size = offset + int(v2.size)\n",
    "            else:\n",
    "                total_size = int(v1.size)\n",
    "            \n",
    "            values = [1.0] * len(indices)\n",
    "            return Vectors.sparse(total_size, sorted(indices), values)\n",
    "            \n",
    "        return udf(process_vectors, VectorUDT())\n",
    "\n",
    "    def train_features(self, items_df):\n",
    "        print(\"building content features (tfidf + bigrams)\")\n",
    "        \n",
    "        df = (\n",
    "            items_df\n",
    "            .withColumn(\"year\", regexp_extract(col(\"title\"), r\"\\((\\d{4})\\)\", 1))\n",
    "            .withColumn(\"decade\", regexp_extract(col(\"title\"), r\"\\((\\d{3})\\d\\)\", 1))\n",
    "            .withColumn(\"genres_spaced\", F.regexp_replace(col(\"genres\"), r\"\\|\", \" \"))\n",
    "            .withColumn(\"content\", concat_ws(\n",
    "                \" \", \n",
    "                col(\"title\"),\n",
    "                col(\"genres_spaced\"), col(\"genres_spaced\"), col(\"genres_spaced\"), col(\"genres_spaced\"),\n",
    "                col(\"year\"), col(\"decade\")\n",
    "            ))\n",
    "        )\n",
    "\n",
    "        stages = []\n",
    "\n",
    "        stages += [\n",
    "            Tokenizer(inputCol=\"content\", outputCol=\"raw_words\"),\n",
    "            StopWordsRemover(inputCol=\"raw_words\", outputCol=\"words\")\n",
    "        ]\n",
    "        \n",
    "        stages += [\n",
    "            HashingTF(inputCol=\"words\", outputCol=\"tf_uni\", numFeatures=self.num_features),\n",
    "            IDF(inputCol=\"tf_uni\", outputCol=\"tfidf_uni\", minDocFreq=1)\n",
    "        ]\n",
    "        \n",
    "        stages += [\n",
    "            NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\"),\n",
    "            HashingTF(inputCol=\"bigrams\", outputCol=\"tf_bi\", numFeatures=self.bigram_features),\n",
    "            IDF(inputCol=\"tf_bi\", outputCol=\"tfidf_bi\", minDocFreq=1)\n",
    "        ]\n",
    "        \n",
    "        pipeline = Pipeline(stages=stages)\n",
    "        self.vector_model = pipeline.fit(df)\n",
    "        features_df = self.vector_model.transform(df)\n",
    "        \n",
    "        combiner = self.combine_binarize_udf()\n",
    "        self.movies_binary = (\n",
    "            features_df\n",
    "            .withColumn(\"binary_features\", combiner(\"tfidf_uni\", \"tfidf_bi\"))\n",
    "            .select(\"item_id\", \"title\", \"genres\", \"binary_features\")\n",
    "            .cache()\n",
    "        )\n",
    "        \n",
    "        print(f\"checked {self.movies_binary.count()} movies\")\n",
    "\n",
    "    def build_lsh_index(self):\n",
    "        print(\"indexing with minhash LSH\")\n",
    "        \n",
    "        mh = MinHashLSH(\n",
    "            inputCol=\"binary_features\", \n",
    "            outputCol=\"hashes\", \n",
    "            numHashTables=self.num_hash_tables,\n",
    "            seed=42\n",
    "        )\n",
    "        self.lsh_model = mh.fit(self.movies_binary)\n",
    "        \n",
    "        dist_threshold = 1.0 - self.jaccard_threshold\n",
    "        \n",
    "        print(f\"cmputing similarity graph\")\n",
    "        raw_pairs = self.lsh_model.approxSimilarityJoin(\n",
    "            self.movies_binary, self.movies_binary, \n",
    "            threshold=dist_threshold, \n",
    "            distCol=\"jaccard_dist\"\n",
    "        )\n",
    "        \n",
    "        pairs = raw_pairs.select(\n",
    "            col(\"datasetA.item_id\").alias(\"item_a\"),\n",
    "            col(\"datasetB.item_id\").alias(\"item_b\"),\n",
    "            (1.0 - col(\"jaccard_dist\")).alias(\"similarity\")\n",
    "        ).filter(\"item_a != item_b\")\n",
    "        \n",
    "        self.similar_pairs = pairs.cache()\n",
    "        print(f\"indexed {self.similar_pairs.count()} similar item pairs\")\n",
    "\n",
    "    def recommend_for_users(self, train_df, items_df, k=10):\n",
    "        print(\"generating content-based recommendations\")\n",
    "        \n",
    "        user_history = train_df.filter(col(\"rating\") >= 4.0).select(\n",
    "            col(\"user_id\"), col(\"item_id\").alias(\"seed_item\"), col(\"rating\")\n",
    "        )\n",
    "\n",
    "        candidates = user_history.join(\n",
    "            self.similar_pairs,\n",
    "            user_history.seed_item == self.similar_pairs.item_a\n",
    "        )\n",
    "        \n",
    "        genre_df = items_df.select(\"item_id\", split(col(\"genres\"), r\"\\|\").alias(\"genres_arr\"))        \n",
    "        \n",
    "        candidates_enriched = (\n",
    "            candidates.alias(\"c\")\n",
    "            .join(\n",
    "                genre_df.alias(\"seed_g\"), \n",
    "                col(\"c.seed_item\") == col(\"seed_g.item_id\")\n",
    "            )\n",
    "            .join(\n",
    "                genre_df.alias(\"cand_g\"), \n",
    "                col(\"c.item_b\") == col(\"cand_g.item_id\")\n",
    "            )\n",
    "            .select(\n",
    "                \"c.user_id\", \n",
    "                col(\"c.item_b\").alias(\"candidate_item\"),\n",
    "                \"c.rating\", \n",
    "                \"c.similarity\",\n",
    "                size(array_intersect(\n",
    "                    col(\"seed_g.genres_arr\"), \n",
    "                    col(\"cand_g.genres_arr\")\n",
    "                )).alias(\"genre_overlap\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        filtered = candidates_enriched.filter(col(\"genre_overlap\") >= self.min_genre_overlap)\n",
    "\n",
    "        scored = filtered.withColumn(\n",
    "            \"score\", \n",
    "            col(\"rating\") * col(\"similarity\") * (1.0 + 0.25 * col(\"genre_overlap\"))\n",
    "        )\n",
    "\n",
    "        recs = scored.groupBy(\"user_id\", \"candidate_item\").agg(_sum(\"score\").alias(\"content_score\"))\n",
    "        \n",
    "        seen_items = train_df.select(\"user_id\", \"item_id\").distinct().alias(\"seen\")\n",
    "        recs_alias = recs.alias(\"recs\")\n",
    "        \n",
    "        final_recs = recs_alias.join(\n",
    "            seen_items,\n",
    "            (col(\"recs.user_id\") == col(\"seen.user_id\")) & \n",
    "            (col(\"recs.candidate_item\") == col(\"seen.item_id\")),\n",
    "            \"left_anti\"\n",
    "        ).select(\n",
    "            col(\"recs.user_id\"), \n",
    "            col(\"recs.candidate_item\").alias(\"item_id\"), \n",
    "            col(\"recs.content_score\")\n",
    "        )\n",
    "\n",
    "        window = Window.partitionBy(\"user_id\").orderBy(desc(\"content_score\"))\n",
    "        return (\n",
    "            final_recs\n",
    "            .withColumn(\"rank\", row_number().over(window))\n",
    "            .filter(col(\"rank\") <= k)\n",
    "            .drop(\"rank\")\n",
    "        )\n",
    "\n",
    "cb_filter = ContentBasedFilter()\n",
    "cb_filter.train_features(items_df)\n",
    "cb_filter.build_lsh_index()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building content features (tfidf + bigrams)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked 3883 movies\n",
      "indexing with minhash LSH\n",
      "cmputing similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 554:===========================>                          (10 + 10) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexed 988212 similar item pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:20.149742Z",
     "start_time": "2026-01-18T18:45:19.973024Z"
    }
   },
   "source": [
    "content_recs = cb_filter.recommend_for_users(train_df, items_df, k=100)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating content-based recommendations\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:45.696191Z",
     "start_time": "2026-01-18T18:45:20.152413Z"
    }
   },
   "source": [
    "content_recs = content_recs.cache()\n",
    "content_recs.show(5)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 572:===========================>                          (10 + 10) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+\n",
      "|user_id|item_id|     content_score|\n",
      "+-------+-------+------------------+\n",
      "|     95|   1744| 22.83967523704366|\n",
      "|     95|   1591|22.473956043956044|\n",
      "|     95|    849|21.873400389932648|\n",
      "|     95|   2334|21.532246786394385|\n",
      "|     95|   2058| 21.53224678639438|\n",
      "+-------+-------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUJnnDrCh518"
   },
   "source": "## 7.Fusion & Evaluation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9PM6sSWqh518",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:45.735460Z",
     "start_time": "2026-01-18T18:45:45.715129Z"
    }
   },
   "source": "ALPHA = 0.7",
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-PgBqkXh518"
   },
   "source": "### 7.1 Normalization"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z3B5QAFnh518",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:45.743388Z",
     "start_time": "2026-01-18T18:45:45.736645Z"
    }
   },
   "source": [
    "def normalize(df, col_name):\n",
    "    stats = df.agg(F.min(col_name).alias(\"min\"), F.max(col_name).alias(\"max\")).collect()[0]\n",
    "    if stats[\"max\"] == stats[\"min\"]:\n",
    "        return df.withColumn(col_name + \"_norm\", F.lit(0.5))\n",
    "    return df.withColumn(col_name + \"_norm\", (F.col(col_name) - stats[\"min\"]) / (stats[\"max\"] - stats[\"min\"]))"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHZure3Wh518"
   },
   "source": "### 7.2 Hybrid Fusion"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "63j6mXT6h518",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:46.438582Z",
     "start_time": "2026-01-18T18:45:45.743901Z"
    }
   },
   "source": [
    "als_norm = normalize(als_recs, \"als_score\")\n",
    "content_norm = normalize(content_recs, \"content_score\")\n",
    "\n",
    "hybrid_recs = als_norm.select(\"user_id\", \"item_id\", \"als_score_norm\") \\\n",
    "    .join(content_norm.select(\"user_id\", \"item_id\", \"content_score_norm\"), [\"user_id\", \"item_id\"], \"full_outer\") \\\n",
    "    .fillna(0) \\\n",
    "    .withColumn(\"final_score\", ALPHA * F.col(\"als_score_norm\") + (1 - ALPHA) * F.col(\"content_score_norm\"))"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEQFjotih518",
    "outputId": "6edb4ce3-e385-4ee9-e167-5ea8474b4f57",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:47.581423Z",
     "start_time": "2026-01-18T18:45:46.485753Z"
    }
   },
   "source": [
    "hybrid_recs.orderBy(F.desc(\"final_score\")).show(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+-------------------+------------------+\n",
      "|user_id|item_id|    als_score_norm| content_score_norm|       final_score|\n",
      "+-------+-------+------------------+-------------------+------------------+\n",
      "|   4277|     53|0.7813458212323617|  0.590465080351217|0.7240815989680184|\n",
      "|   2441|   3851|               1.0|                0.0|               0.7|\n",
      "|   5246|     69|0.9988184773816583|                0.0|0.6991729341671608|\n",
      "|   1812|   2305|0.8879036352052065|0.24769294234208108|0.6958404273462688|\n",
      "|   1200|   2964|0.9780825664404157|0.03430897380639135|0.6949504886502084|\n",
      "|    210|   2571|0.9030695940179349| 0.2009174378415168|0.6924239471650094|\n",
      "|   4277|   2579|0.7558030874462422| 0.5430159889973912|0.6919669579115869|\n",
      "|   3539|    635|0.7296950332243268| 0.6024709281442315|0.6915278017002983|\n",
      "|   1111|   3847|0.9842770556661816|                0.0|0.6889939389663271|\n",
      "|   3032|   2332|0.9836119100891428|                0.0|0.6885283370623999|\n",
      "+-------+-------+------------------+-------------------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:48.328927Z",
     "start_time": "2026-01-18T18:45:47.616251Z"
    }
   },
   "source": [
    "hybrid_recs.orderBy(F.desc(\"final_score\")).filter((F.col(\"als_score_norm\") > 0.2) & (F.col(\"content_score_norm\") > 0.2)).show(10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+-------------------+------------------+\n",
      "|user_id|item_id|    als_score_norm| content_score_norm|       final_score|\n",
      "+-------+-------+------------------+-------------------+------------------+\n",
      "|   4277|     53|0.7813458212323617|  0.590465080351217|0.7240815989680184|\n",
      "|   1812|   2305|0.8879036352052065|0.24769294234208108|0.6958404273462688|\n",
      "|    210|   2571|0.9030695940179349| 0.2009174378415168|0.6924239471650094|\n",
      "|   4277|   2579|0.7558030874462422| 0.5430159889973912|0.6919669579115869|\n",
      "|   3539|    635|0.7296950332243268| 0.6024709281442315|0.6915278017002983|\n",
      "|   4086|   3159|  0.89033927173392| 0.2142719162945873|0.6875190651021201|\n",
      "|   4028|   2858|0.8366232663135118| 0.3306435727439082|0.6848293582426307|\n",
      "|   1448|     53|0.7366513918047629| 0.5233646290001703|0.6726653629633851|\n",
      "|    195|     53|0.7981084007992989|0.36953719846743455|0.6695370400997396|\n",
      "|   1835|     53|0.7863732285641016|   0.39215405067471|0.6681074751972841|\n",
      "+-------+-------+------------------+-------------------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:49.592672Z",
     "start_time": "2026-01-18T18:45:48.366661Z"
    }
   },
   "source": [
    "als_pairs = als_recs.select(\"user_id\", \"item_id\").distinct()\n",
    "content_pairs = content_recs.select(\"user_id\", \"item_id\").distinct()\n",
    "\n",
    "overlap = als_pairs.intersect(content_pairs).count()\n",
    "print(f\"als pairs: {als_pairs.count()}\")\n",
    "print(f\"content pairs: {content_pairs.count()}\")\n",
    "print(f\"overlap: {overlap}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "als pairs: 604000\n",
      "content pairs: 603761\n",
      "overlap: 12453\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcQId4lLh52C"
   },
   "source": "### 7.3 Evaluation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4_55bcPh52C",
    "outputId": "6b5486e9-a788-4a38-b856-37791abbc479",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:51.028632Z",
     "start_time": "2026-01-18T18:45:49.604580Z"
    }
   },
   "source": [
    "als_metrics = evaluate(als_recs, \"als_score\", \"ALS\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS: P@10=0.0281, R@10=0.0205, NDCG@10=0.0307\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "La8eg0aFh52D",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:52.099975Z",
     "start_time": "2026-01-18T18:45:51.038460Z"
    }
   },
   "source": [
    "content_metrics = evaluate(content_recs, \"content_score\", \"Content-Based\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content-Based: P@10=0.0150, R@10=0.0167, NDCG@10=0.0188\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9hpFNv0Vh52D",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:55.080163Z",
     "start_time": "2026-01-18T18:45:52.109796Z"
    }
   },
   "source": [
    "hybrid_metrics = evaluate(hybrid_recs, \"final_score\", \"Hybrid\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid: P@10=0.0292, R@10=0.0215, NDCG@10=0.0324\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kL3phVEuh52D"
   },
   "source": [
    "### Bonus: GBT Re-Ranking"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eZfDCdK1h52D",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:55.098519Z",
     "start_time": "2026-01-18T18:45:55.091054Z"
    }
   },
   "source": [
    "# TODO"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqAeFwu6h52D"
   },
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FlHvgvrWh52D",
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:56.494982Z",
     "start_time": "2026-01-18T18:45:55.098838Z"
    }
   },
   "source": [
    "summary = [\n",
    "    (\"ALS\", als_metrics[\"Precision@10\"], als_metrics[\"Recall@10\"], als_metrics[\"NDCG@10\"]),\n",
    "    (\"Content-Based\", content_metrics[\"Precision@10\"], content_metrics[\"Recall@10\"], content_metrics[\"NDCG@10\"]),\n",
    "    (\"Hybrid\", hybrid_metrics[\"Precision@10\"], hybrid_metrics[\"Recall@10\"], hybrid_metrics[\"NDCG@10\"]),\n",
    "]\n",
    "spark.createDataFrame(summary, [\"Model\", \"Precision@10\", \"Recall@10\", \"NDCG@10\"]).show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|        Model|        Precision@10|           Recall@10|             NDCG@10|\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|          ALS|0.028066945606694073|0.020494325036241722|0.030663505538513897|\n",
      "|Content-Based| 0.01497907949790795|0.016735285093672912| 0.01879822686955161|\n",
      "|       Hybrid|0.029205020920502162| 0.02149253217102322| 0.03244395940155893|\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:45:58.247457Z",
     "start_time": "2026-01-18T18:45:56.532622Z"
    }
   },
   "source": [
    "# Sample User Recommendations Visualization\n",
    "import random\n",
    "\n",
    "# Pick a random user from test set\n",
    "test_users = test_df.select(\"user_id\").distinct().collect()\n",
    "sample_user_id = random.choice(test_users)[\"user_id\"]\n",
    "print(f\"Sample User ID: {sample_user_id}\\n\")\n",
    "\n",
    "# Get user's highly-rated movies from training set (rating >= 4)\n",
    "user_liked = train_df.filter(\n",
    "    (F.col(\"user_id\") == sample_user_id) & (F.col(\"rating\") >= 4.0)\n",
    ").join(items_df, \"item_id\").select(\"title\", \"genres\", \"rating\").orderBy(F.desc(\"rating\"))\n",
    "\n",
    "print(\"=== Movies rated highly by this user (training data) ===\")\n",
    "user_liked.show(10, truncate=False)\n",
    "\n",
    "# Get hybrid recommendations for this user\n",
    "user_recs = hybrid_recs.filter(F.col(\"user_id\") == sample_user_id) \\\n",
    "    .join(items_df, \"item_id\") \\\n",
    "    .select(\"title\", \"genres\", \"final_score\") \\\n",
    "    .orderBy(F.desc(\"final_score\"))\n",
    "\n",
    "print(\"=== Hybrid model recommendations ===\")\n",
    "user_recs.show(10, truncate=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample User ID: 4598\n",
      "\n",
      "=== Movies rated highly by this user (training data) ===\n",
      "+--------------------------------------+---------------------------+------+\n",
      "|title                                 |genres                     |rating|\n",
      "+--------------------------------------+---------------------------+------+\n",
      "|Anatomy of a Murder (1959)            |Drama|Mystery              |5.0   |\n",
      "|Best Years of Our Lives, The (1946)   |Drama|War                  |5.0   |\n",
      "|Silence of the Lambs, The (1991)      |Drama|Thriller             |5.0   |\n",
      "|Rosewood (1997)                       |Drama                      |5.0   |\n",
      "|Bridge on the River Kwai, The (1957)  |Drama|War                  |5.0   |\n",
      "|Babe (1995)                           |Children's|Comedy|Drama    |5.0   |\n",
      "|Pulp Fiction (1994)                   |Crime|Drama                |5.0   |\n",
      "|Toy Story (1995)                      |Animation|Children's|Comedy|5.0   |\n",
      "|One Flew Over the Cuckoo's Nest (1975)|Drama                      |5.0   |\n",
      "|North by Northwest (1959)             |Drama|Thriller             |5.0   |\n",
      "+--------------------------------------+---------------------------+------+\n",
      "only showing top 10 rows\n",
      "=== Hybrid model recommendations ===\n",
      "+-----------------------------------------+-------------------------------+-------------------+\n",
      "|title                                    |genres                         |final_score        |\n",
      "+-----------------------------------------+-------------------------------+-------------------+\n",
      "|Schindler's List (1993)                  |Drama|War                      |0.5335609217330187 |\n",
      "|Shawshank Redemption, The (1994)         |Drama                          |0.4971454661649952 |\n",
      "|Silence of the Lambs, The (1991)         |Drama|Thriller                 |0.4943139987218505 |\n",
      "|Pulp Fiction (1994)                      |Crime|Drama                    |0.49187113213921785|\n",
      "|Bridge on the River Kwai, The (1957)     |Drama|War                      |0.48506023720391905|\n",
      "|Sophie's Choice (1982)                   |Drama                          |0.4836622103302157 |\n",
      "|Star Wars: Episode IV - A New Hope (1977)|Action|Adventure|Fantasy|Sci-Fi|0.48243399280599936|\n",
      "|One Flew Over the Cuckoo's Nest (1975)   |Drama                          |0.4766767570823999 |\n",
      "|Raisin in the Sun, A (1961)              |Drama                          |0.4704698624021883 |\n",
      "|Raiders of the Lost Ark (1981)           |Action|Adventure               |0.4701244771052492 |\n",
      "+-----------------------------------------+-------------------------------+-------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T18:49:32.076753Z",
     "start_time": "2026-01-18T18:45:58.289702Z"
    }
   },
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import Evaluator\n",
    "from pyspark.ml import Estimator, Model\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "import time\n",
    "\n",
    "class ContentBasedEstimator(Estimator, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    numFeatures = Param(Params._dummy(), \"numFeatures\", \"num of tf-idf features\", TypeConverters.toInt)\n",
    "    bigramFeatures = Param(Params._dummy(), \"bigramFeatures\", \"num of bigram features\", TypeConverters.toInt)\n",
    "    numHashTables = Param(Params._dummy(), \"numHashTables\", \"num of LSH hash tables\", TypeConverters.toInt)\n",
    "    jaccardThreshold = Param(Params._dummy(), \"jaccardThreshold\", \"jaccard similarity threshold\", TypeConverters.toFloat)\n",
    "    minGenreOverlap = Param(Params._dummy(), \"minGenreOverlap\", \"minimum genre overlap\", TypeConverters.toInt)\n",
    "    \n",
    "    def __init__(self, numFeatures=5000, bigramFeatures=3000, \n",
    "                numHashTables=10, jaccardThreshold=0.1, minGenreOverlap=1):\n",
    "        super(ContentBasedEstimator, self).__init__()\n",
    "        self._setDefault(numFeatures=numFeatures, \n",
    "                        bigramFeatures=bigramFeatures, \n",
    "                        numHashTables=numHashTables,\n",
    "                        jaccardThreshold=jaccardThreshold,\n",
    "                        minGenreOverlap=minGenreOverlap)\n",
    "        \n",
    "        self._set(numFeatures=numFeatures, \n",
    "                bigramFeatures=bigramFeatures, \n",
    "                numHashTables=numHashTables,\n",
    "                jaccardThreshold=jaccardThreshold, \n",
    "                minGenreOverlap=minGenreOverlap)\n",
    "    \n",
    "    def _fit(self, dataset):\n",
    "        cb = ContentBasedFilter(\n",
    "            num_features=self.getOrDefault(self.numFeatures),\n",
    "            bigram_features=self.getOrDefault(self.bigramFeatures),\n",
    "            num_hash_tables=self.getOrDefault(self.numHashTables)\n",
    "        )\n",
    "        cb.jaccard_threshold = self.getOrDefault(self.jaccardThreshold)\n",
    "        cb.min_genre_overlap = self.getOrDefault(self.minGenreOverlap)\n",
    "        cb.train_features(items_df)\n",
    "        cb.build_lsh_index()\n",
    "        return ContentBasedModel(cb)\n",
    "\n",
    "class ContentBasedModel(Model, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self, cb_filter=None):\n",
    "        super(ContentBasedModel, self).__init__()\n",
    "        self.cb_filter = cb_filter\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        if self.cb_filter is None:\n",
    "            raise ValueError(\"model not fitted\")\n",
    "        return self.cb_filter.recommend_for_users(dataset, items_df, k=100)\n",
    "\n",
    "class RecSysEvaluator(Evaluator):\n",
    "    def __init__(self, test_df, ground_truth, k=10):\n",
    "        super(RecSysEvaluator, self).__init__()\n",
    "        self.test_df = test_df\n",
    "        self.ground_truth = ground_truth\n",
    "        self.k = k\n",
    "    \n",
    "    def _evaluate(self, dataset):\n",
    "        top_k = get_top_k(dataset, \"content_score\", self.k)\n",
    "        return ndcg_at_k(top_k, self.ground_truth, self.k)\n",
    "    \n",
    "    def isLargerBetter(self):\n",
    "        return True\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(ContentBasedEstimator.numFeatures, [3000, 5000, 7000]) \\\n",
    "    .addGrid(ContentBasedEstimator.bigramFeatures, [2000, 3000]) \\\n",
    "    .addGrid(ContentBasedEstimator.numHashTables, [10, 20, 30]) \\\n",
    "    .addGrid(ContentBasedEstimator.jaccardThreshold, [0.1, 0.2]) \\\n",
    "    .addGrid(ContentBasedEstimator.minGenreOverlap, [1, 2]) \\\n",
    "    .build()\n",
    "\n",
    "estimator = ContentBasedEstimator()\n",
    "evaluator = RecSysEvaluator(test_df, ground_truth, k=10)\n",
    "items_df.cache()\n",
    "train_df.cache()\n",
    "cv = CrossValidator(estimator=estimator, estimatorParamMaps=paramGrid, \n",
    "                    evaluator=evaluator, numFolds=2, parallelism=10)\n",
    "\n",
    "start = time.time()\n",
    "cvModel = cv.fit(train_df)\n",
    "train_time = time.time() - start\n",
    "\n",
    "best_model = cvModel.bestModel\n",
    "print(f\"Training time: {train_time:.2f}s\")\n",
    "print(f\"Best params: numFeatures={best_model.cb_filter.num_features}, bigramFeatures={best_model.cb_filter.bigram_features}, numHashTables={best_model.cb_filter.num_hash_tables}, jaccardThreshold={best_model.cb_filter.jaccard_threshold:.3f}, minGenreOverlap={best_model.cb_filter.min_genre_overlap}\")\n",
    "\n",
    "start = time.time()\n",
    "tuned_recs = best_model.transform(train_df).cache()\n",
    "inference_time = time.time() - start\n",
    "\n",
    "tuned_metrics = evaluate(tuned_recs, \"content_score\", \"Tuned Content-Based\")\n",
    "print(f\"Inference time: {inference_time:.2f}s\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
      "building content features (tfidf + bigrams)\n",
      "building content features (tfidf + bigrams)\n",
      "\n",
      "building content features (tfidf + bigrams)\n",
      "building content features (tfidf + bigrams)\n",
      "building content features (tfidf + bigrams)\n",
      "building content features (tfidf + bigrams)\n",
      "building content features (tfidf + bigrams)\n",
      "building content features (tfidf + bigrams)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/18 20:45:58 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked 3883 movies\n",
      "indexing with minhash LSH\n",
      "cmputing similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checked 3883 movies\n",
      "indexing with minhash LSH\n",
      "checked 3883 movies\n",
      "indexing with minhash LSH\n",
      "cmputing similarity graph\n",
      "cmputing similarity graph\n",
      "checked 3883 movies\n",
      "indexing with minhash LSH\n",
      "cmputing similarity graph\n",
      "checked 3883 movies\n",
      "indexing with minhash LSH\n",
      "checked 3883 movies\n",
      "indexing with minhash LSH\n",
      "cmputing similarity graph\n",
      "cmputing similarity graph\n",
      "checked 3883 movies\n",
      "indexing with minhash LSH\n",
      "checked 3883 movies\n",
      "indexing with minhash LSH\n",
      "checked 3883 movies\n",
      "indexing with minhash LSH\n",
      "cmputing similarity graph\n",
      "cmputing similarity graph\n",
      "cmputing similarity graph\n",
      "checked 3883 movies\n",
      "indexing with minhash LSH\n",
      "cmputing similarity graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexed 988212 similar item pairs\n",
      "generating content-based recommendations\n",
      "indexed 988212 similar item pairs\n",
      "indexed 988212 similar item pairs\n",
      "generating content-based recommendations\n",
      "indexed 988212 similar item pairs\n",
      "generating content-based recommendations\n",
      "generating content-based recommendations\n",
      "indexed 988212 similar item pairs\n",
      "generating content-based recommendations\n",
      "indexed 988212 similar item pairs\n",
      "generating content-based recommendations\n",
      "indexed 988212 similar item pairs\n",
      "generating content-based recommendations\n",
      "indexed 988212 similar item pairs\n",
      "generating content-based recommendations\n",
      "indexed 988212 similar item pairs\n",
      "generating content-based recommendations\n",
      "indexed 988212 similar item pairs\n",
      "generating content-based recommendations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3595:(0 + 10) / 20][Stage 3597:>(0 + 0) / 20][Stage 3599:>(0 + 0) / 20]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/envs/mmds/lib/python3.10/multiprocessing/pool.py:856\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    855\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 856\u001B[0m     item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_items\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpopleft\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    857\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m:\n",
      "\u001B[0;31mIndexError\u001B[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 82\u001B[0m\n\u001B[1;32m     78\u001B[0m cv \u001B[38;5;241m=\u001B[39m CrossValidator(estimator\u001B[38;5;241m=\u001B[39mestimator, estimatorParamMaps\u001B[38;5;241m=\u001B[39mparamGrid, \n\u001B[1;32m     79\u001B[0m                     evaluator\u001B[38;5;241m=\u001B[39mevaluator, numFolds\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, parallelism\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m     81\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m---> 82\u001B[0m cvModel \u001B[38;5;241m=\u001B[39m \u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_df\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     83\u001B[0m train_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start\n\u001B[1;32m     85\u001B[0m best_model \u001B[38;5;241m=\u001B[39m cvModel\u001B[38;5;241m.\u001B[39mbestModel\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mmds/lib/python3.10/site-packages/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    208\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mmds/lib/python3.10/site-packages/pyspark/ml/tuning.py:863\u001B[0m, in \u001B[0;36mCrossValidator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    856\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _cache_spark_dataset(train) \u001B[38;5;28;01mas\u001B[39;00m train, _cache_spark_dataset(\n\u001B[1;32m    857\u001B[0m     validation\n\u001B[1;32m    858\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m validation:\n\u001B[1;32m    859\u001B[0m     tasks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(\n\u001B[1;32m    860\u001B[0m         inheritable_thread_target(dataset\u001B[38;5;241m.\u001B[39msparkSession),\n\u001B[1;32m    861\u001B[0m         _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001B[1;32m    862\u001B[0m     )\n\u001B[0;32m--> 863\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j, metric, subModel \u001B[38;5;129;01min\u001B[39;00m pool\u001B[38;5;241m.\u001B[39mimap_unordered(\u001B[38;5;28;01mlambda\u001B[39;00m f: f(), tasks):\n\u001B[1;32m    864\u001B[0m         metrics_all[i][j] \u001B[38;5;241m=\u001B[39m metric\n\u001B[1;32m    865\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m collectSubModelsParam:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mmds/lib/python3.10/multiprocessing/pool.py:861\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    859\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 861\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    862\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    863\u001B[0m     item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_items\u001B[38;5;241m.\u001B[39mpopleft()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mmds/lib/python3.10/threading.py:320\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 320\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "26/01/17 19:33:09 WARN CacheManager: Asked to cache already cached data.\n",
    "26/01/17 19:33:09 WARN CacheManager: Asked to cache already cached data.\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairsindexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "building content features (tfidf + bigrams)\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "building content features (tfidf + bigrams)\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "checked 3883 moviesgenerating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "\n",
    "indexing with minhash LSH\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "building content features (tfidf + bigrams)\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendationsgenerating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "\n",
    "generating content-based recommendations\n",
    "indexing with minhash LSH\n",
    "generating content-based recommendations\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "building content features (tfidf + bigrams)\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "indexed 988212 similar item pairsindexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "building content features (tfidf + bigrams)\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 moviesindexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "checked 3883 movies\n",
    "indexed 988212 similar item pairs\n",
    "\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "indexing with minhash LSH\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graphcmputing similarity graph\n",
    "\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "indexed 988212 similar item pairsindexed 988212 similar item pairs\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "generating content-based recommendations\n",
    "\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "...\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n",
    "                                                                                \n",
    "indexed 988212 similar item pairsindexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "building content features (tfidf + bigrams)\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "cmputing similarity graph\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "indexed 988212 similar item pairsindexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "...\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movieschecked 3883 movies\n",
    "indexing with minhash LSH\n",
    "\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graphcmputing similarity graph\n",
    "\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "indexed 988212 similar item pairsindexed 988212 similar item pairs\n",
    "\n",
    "indexed 988212 similar item pairs\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "                                                                                \n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "building content features (tfidf + bigrams)\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "checked 3883 movieschecked 3883 movies\n",
    "indexing with minhash LSH\n",
    "\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "building content features (tfidf + bigrams)\n",
    "                                                                                \n",
    "checked 3883 movies\n",
    "indexing with minhash LSH\n",
    "cmputing similarity graph\n",
    "                                                                                \n",
    "indexed 988212 similar item pairs\n",
    "Training time: 5721.41s\n",
    "Best params: numFeatures=5000, bigramFeatures=3000, numHashTables=10, jaccardThreshold=0.100, minGenreOverlap=1\n",
    "generating content-based recommendations\n",
    "                                                                                \n",
    "Tuned Content-Based: P@10=0.0150, R@10=0.0167, NDCG@10=0.0188\n",
    "Inference time: 0.09s\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "history_visible": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mining-massive-databases",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
